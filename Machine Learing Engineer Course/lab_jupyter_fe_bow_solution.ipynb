{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "95e483da-23b3-41f0-ab78-5a3ba2e558b3",
   "metadata": {},
   "source": [
    "<p style=\"text-align:center\">\n",
    "    <a href=\"https://skills.network/?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMML321ENSkillsNetwork817-2022-01-01\" target=\"_blank\">\n",
    "    <img src=\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/assets/logos/SN_web_lightmode.png\" width=\"200\" alt=\"Skills Network Logo\"  />\n",
    "    </a>\n",
    "</p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc484e05-7073-47ad-a3ce-e20323f6eedb",
   "metadata": {},
   "source": [
    "# **Extract Bag of Words (BoW) Features from Course Textual Content**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0971929e-91aa-4d34-977d-b74db663cc49",
   "metadata": {},
   "source": [
    "Estimated time needed: **60** minutes\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "564f3ee0-33f5-44f1-8edb-5da393bff1b7",
   "metadata": {},
   "source": [
    "The main goal of recommender systems is to help users find items they potentially interested in. Depending on the recommendation tasks, an item can be a movie, a restaurant, or, in our case, an online course. \n",
    "\n",
    "Machine learning algorithms cannot work on an item directly so we first need to extract features and represent the items mathematically, i.e., with a feature vector.\n",
    "\n",
    "Many items are often described by text so they are associated with textual data, such as the titles and descriptions of a movie or course. Since machine learning algorithms can not process textual data directly, we need to transform the raw text into numeric feature vectors.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "075dfbc7-11d6-4825-95e9-418078f7dce3",
   "metadata": {},
   "source": [
    "![](https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBM-ML321EN-SkillsNetwork/labs/module_2/images/extract_textual_features.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24cfeb45-7ebb-49f7-855f-b6941ce1a0b3",
   "metadata": {},
   "source": [
    "In this lab, you will be learning to extract the bag of words (BoW) features from course titles and descriptions. The BoW feature is a simple but effective feature characterizing textual data and is widely used in many textual machine learning tasks.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83357fce-32db-499f-ad00-897a62180f02",
   "metadata": {},
   "source": [
    "## Objectives\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2f3932b-164a-4103-b631-dbc2e01c048d",
   "metadata": {},
   "source": [
    "After completing this lab you will be able to:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "012219f3-00f1-43d1-b1ba-1f0f61fe4453",
   "metadata": {},
   "source": [
    "* Extract Bag of Words (BoW) features from course titles and descriptions\n",
    "* Build a course BoW dataset to be used for building a content-based recommender system later\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3d70242-7ed4-4761-8283-121328114c13",
   "metadata": {},
   "source": [
    "----\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "059d20cb-9e49-41e6-9fee-9268da4aa539",
   "metadata": {},
   "source": [
    "## Prepare and setup the lab environment\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07fa941b-9108-4148-8534-82797b7c708f",
   "metadata": {},
   "source": [
    "First, let's install and import required libraries:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bfb15844-5f36-4a88-8f3e-249947e73eb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk==3.6.7 in c:\\users\\jato daniel\\appdata\\roaming\\python\\python38\\site-packages (3.6.7)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\jato daniel\\anaconda3\\lib\\site-packages (from nltk==3.6.7) (2023.5.5)\n",
      "Requirement already satisfied: click in c:\\users\\jato daniel\\anaconda3\\lib\\site-packages (from nltk==3.6.7) (7.1.2)\n",
      "Requirement already satisfied: tqdm in c:\\users\\jato daniel\\anaconda3\\lib\\site-packages (from nltk==3.6.7) (4.59.0)\n",
      "Requirement already satisfied: joblib in c:\\users\\jato daniel\\anaconda3\\lib\\site-packages (from nltk==3.6.7) (1.0.1)\n",
      "Requirement already satisfied: gensim==4.1.2 in c:\\users\\jato daniel\\anaconda3\\lib\\site-packages (4.1.2)\n",
      "Requirement already satisfied: smart-open>=1.8.1 in c:\\users\\jato daniel\\anaconda3\\lib\\site-packages (from gensim==4.1.2) (5.2.1)\n",
      "Requirement already satisfied: numpy>=1.17.0 in c:\\users\\jato daniel\\anaconda3\\lib\\site-packages (from gensim==4.1.2) (1.22.4)\n",
      "Requirement already satisfied: scipy>=0.18.1 in c:\\users\\jato daniel\\anaconda3\\lib\\site-packages (from gensim==4.1.2) (1.6.2)\n",
      "Requirement already satisfied: Cython==0.29.23 in c:\\users\\jato daniel\\anaconda3\\lib\\site-packages (from gensim==4.1.2) (0.29.23)\n"
     ]
    }
   ],
   "source": [
    "!pip install nltk==3.6.7 --user\n",
    "!pip install gensim==4.1.2 --user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d17368a0-e7e3-42b3-8a7d-d6db5c500f28",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "import pandas as pd\n",
    "import nltk as nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from gensim import corpora\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b29cbb7-4058-4a6c-b830-c7ecbc7c5313",
   "metadata": {},
   "source": [
    "Download stopwords\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bd9b71d0-1159-4cd6-be41-9e8387f66f69",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to C:\\Users\\JATO\n",
      "[nltk_data]     DANIEL\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt.zip.\n",
      "[nltk_data] Downloading package stopwords to C:\\Users\\JATO\n",
      "[nltk_data]     DANIEL\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\stopwords.zip.\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\JATO DANIEL\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping taggers\\averaged_perceptron_tagger.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d3d003ea-1745-4f59-8fe3-d65844248f56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# also set a random state\n",
    "rs = 123"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "740312d5-23b7-40c6-a2cb-a03a340f4523",
   "metadata": {},
   "source": [
    "### Bag of Words (BoW) features\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0e5fd8e-0ed5-42a6-9415-b240e52ab70e",
   "metadata": {},
   "source": [
    "BoW features are essentially the counts or frequencies of each word that appears in a text (string). Let's illustrate it with some simple examples.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ee872c3-40f4-4baa-b6e7-c13f2873d823",
   "metadata": {},
   "source": [
    "Suppose we have two course descriptions as follows:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4ed67ed3-608e-4e14-9f69-1d31ae0595db",
   "metadata": {},
   "outputs": [],
   "source": [
    "course1 = \"this is an introduction data science course which introduces data science to beginners\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e52596e0-a377-4594-9150-677e9afcb9a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "course2 = \"machine learning for beginners\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b53f5476-0fbf-4946-aafd-5e9ae4ecfb7d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['this is an introduction data science course which introduces data science to beginners',\n",
       " 'machine learning for beginners']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "courses = [course1, course2]\n",
    "courses"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae39e1f2-7a3c-4b9d-ab20-75030f8e447c",
   "metadata": {},
   "source": [
    "The first step is to split the two strings into words (tokens). A token in the text processing context means the smallest unit of text such as a word, a symbol/punctuation, or a phrase, etc. The process to transform a string into a collection of tokens is called `tokenization`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7035267f-e19d-4b86-842d-f5e35a2991e3",
   "metadata": {},
   "source": [
    "One common way to do ```tokenization``` is to use the Python built-in `split()` method of the `str` class.  However, in this lab, we want to leverage the `nltk` (Natural Language Toolkit) package, which is probably the most commonly used package to process text or natural language.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00e51374-3337-4b4d-a0d4-c91eed6d442d",
   "metadata": {},
   "source": [
    " More specifically, we will use the ```word_tokenize()``` method on the content of course (string):\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ab0d204f-e87c-43f3-a118-c50fc43418c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize the two courses\n",
    "tokenized_courses = [word_tokenize(course) for course in courses]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7baaecde-2e48-49f8-b098-62bd854e70d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['this',\n",
       "  'is',\n",
       "  'an',\n",
       "  'introduction',\n",
       "  'data',\n",
       "  'science',\n",
       "  'course',\n",
       "  'which',\n",
       "  'introduces',\n",
       "  'data',\n",
       "  'science',\n",
       "  'to',\n",
       "  'beginners'],\n",
       " ['machine', 'learning', 'for', 'beginners']]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_courses"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2f426d5-63dc-480d-ac87-d9f24e8be5fe",
   "metadata": {},
   "source": [
    "As you can see from the cell output, two courses have been tokenized and turned into two token arrays.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15c7fa17-dc8c-4279-88dd-5fa91e559500",
   "metadata": {},
   "source": [
    "Next, we want to create a token dictionary to index all tokens. Basically, we want to assign a key/index for each token. One way to index tokens is to use the `gensim` package which is another popular package for processing textual data:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5aa71f1b-c537-433c-81d9-7be19cb9dc50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a token dictionary for the two courses\n",
    "tokens_dict = gensim.corpora.Dictionary(tokenized_courses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e70dedf7-5c0d-49af-9414-ed39072ba468",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'an': 0, 'beginners': 1, 'course': 2, 'data': 3, 'introduces': 4, 'introduction': 5, 'is': 6, 'science': 7, 'this': 8, 'to': 9, 'which': 10, 'for': 11, 'learning': 12, 'machine': 13}\n"
     ]
    }
   ],
   "source": [
    "print(tokens_dict.token2id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "684cafae-cbe0-4959-a593-4254955063c3",
   "metadata": {},
   "source": [
    "With the token dictionary, we can easily count each token in the two example courses and output two BoW feature vectors. However, more conveniently, the `gensim` package provides us a `doc2bow` method to generate BoW features out-of-box.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b92058f4-d73e-43a4-aa91-44269f427c20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate BoW features for each course\n",
    "courses_bow = [tokens_dict.doc2bow(course) for course in tokenized_courses]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "709ef8d4-49bc-4aec-aa94-213a93a86c59",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[(0, 1),\n",
       "  (1, 1),\n",
       "  (2, 1),\n",
       "  (3, 2),\n",
       "  (4, 1),\n",
       "  (5, 1),\n",
       "  (6, 1),\n",
       "  (7, 2),\n",
       "  (8, 1),\n",
       "  (9, 1),\n",
       "  (10, 1)],\n",
       " [(1, 1), (11, 1), (12, 1), (13, 1)]]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "courses_bow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6610ac6e-536a-4574-b2f3-0347f0c3b225",
   "metadata": {},
   "source": [
    "It outputs two BoW arrays where each element is a tuple, e.g., (0, 1) and (7, 2). The first element of the tuple is the token ID and the second element is its count. So `(0, 1)` means `(``an``, 1)` and `(7, 2)` means `(``science``, 2)`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96c6274b-f257-4627-a509-95a572ab1faa",
   "metadata": {},
   "source": [
    "We can use the following code snippet to print each token and its count:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "06ad83c6-ed33-4182-9fbd-c47cb53ff8f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bag of words for course 0:\n",
      "--Token: 'an', Count:1\n",
      "--Token: 'beginners', Count:1\n",
      "--Token: 'course', Count:1\n",
      "--Token: 'data', Count:2\n",
      "--Token: 'introduces', Count:1\n",
      "--Token: 'introduction', Count:1\n",
      "--Token: 'is', Count:1\n",
      "--Token: 'science', Count:2\n",
      "--Token: 'this', Count:1\n",
      "--Token: 'to', Count:1\n",
      "--Token: 'which', Count:1\n",
      "Bag of words for course 1:\n",
      "--Token: 'beginners', Count:1\n",
      "--Token: 'for', Count:1\n",
      "--Token: 'learning', Count:1\n",
      "--Token: 'machine', Count:1\n"
     ]
    }
   ],
   "source": [
    "for course_idx, course_bow in enumerate(courses_bow):\n",
    "    print(f\"Bag of words for course {course_idx}:\")\n",
    "    # For each token index, print its bow value (word count)\n",
    "    for token_index, token_bow in course_bow:\n",
    "        token = tokens_dict.get(token_index)\n",
    "        print(f\"--Token: '{token}', Count:{token_bow}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe609836-62a6-4837-9063-d4424a7fcab3",
   "metadata": {},
   "source": [
    "If we turn to the long list into a horizontal feature vectors, we can see the two courses become two numerical feature vectors:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "155b7c02-f768-4dc5-81e2-a8c404d753b4",
   "metadata": {},
   "source": [
    "![](https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBM-ML321EN-SkillsNetwork/labs/module_2/images/bow.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f840ee20-5efa-4d2f-998d-c59f78d6fddd",
   "metadata": {},
   "source": [
    "### BoW dimensionality reduction\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35d9b92b-c25b-4f7e-82de-168a89e93f95",
   "metadata": {},
   "source": [
    "A document may contain tens of thousands of words which makes the dimension of the BoW feature vector huge. To reduce the dimensionality, one common way is to filter the relatively meaningless tokens such as stop words or sometimes add position and adjective words.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d1618a2-9e99-48ab-aec8-a3401b257296",
   "metadata": {},
   "source": [
    "Note there are many other ways to reduce dimensionality such as `stemming` and `lemmatization` but they are beyond the scope of this capstone project. You are encouraged to explore them yourself.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dc8c1e2-ecfa-4d51-b066-ee37ce57c871",
   "metadata": {},
   "source": [
    "We can use the english stop words provided in `nltk`:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6bdd93bb-9a44-4918-9c8b-61c35e3770df",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "21ef0d1c-b391-424d-895a-117aa58d32ba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'a',\n",
       " 'about',\n",
       " 'above',\n",
       " 'after',\n",
       " 'again',\n",
       " 'against',\n",
       " 'ain',\n",
       " 'all',\n",
       " 'am',\n",
       " 'an',\n",
       " 'and',\n",
       " 'any',\n",
       " 'are',\n",
       " 'aren',\n",
       " \"aren't\",\n",
       " 'as',\n",
       " 'at',\n",
       " 'be',\n",
       " 'because',\n",
       " 'been',\n",
       " 'before',\n",
       " 'being',\n",
       " 'below',\n",
       " 'between',\n",
       " 'both',\n",
       " 'but',\n",
       " 'by',\n",
       " 'can',\n",
       " 'couldn',\n",
       " \"couldn't\",\n",
       " 'd',\n",
       " 'did',\n",
       " 'didn',\n",
       " \"didn't\",\n",
       " 'do',\n",
       " 'does',\n",
       " 'doesn',\n",
       " \"doesn't\",\n",
       " 'doing',\n",
       " 'don',\n",
       " \"don't\",\n",
       " 'down',\n",
       " 'during',\n",
       " 'each',\n",
       " 'few',\n",
       " 'for',\n",
       " 'from',\n",
       " 'further',\n",
       " 'had',\n",
       " 'hadn',\n",
       " \"hadn't\",\n",
       " 'has',\n",
       " 'hasn',\n",
       " \"hasn't\",\n",
       " 'have',\n",
       " 'haven',\n",
       " \"haven't\",\n",
       " 'having',\n",
       " 'he',\n",
       " 'her',\n",
       " 'here',\n",
       " 'hers',\n",
       " 'herself',\n",
       " 'him',\n",
       " 'himself',\n",
       " 'his',\n",
       " 'how',\n",
       " 'i',\n",
       " 'if',\n",
       " 'in',\n",
       " 'into',\n",
       " 'is',\n",
       " 'isn',\n",
       " \"isn't\",\n",
       " 'it',\n",
       " \"it's\",\n",
       " 'its',\n",
       " 'itself',\n",
       " 'just',\n",
       " 'll',\n",
       " 'm',\n",
       " 'ma',\n",
       " 'me',\n",
       " 'mightn',\n",
       " \"mightn't\",\n",
       " 'more',\n",
       " 'most',\n",
       " 'mustn',\n",
       " \"mustn't\",\n",
       " 'my',\n",
       " 'myself',\n",
       " 'needn',\n",
       " \"needn't\",\n",
       " 'no',\n",
       " 'nor',\n",
       " 'not',\n",
       " 'now',\n",
       " 'o',\n",
       " 'of',\n",
       " 'off',\n",
       " 'on',\n",
       " 'once',\n",
       " 'only',\n",
       " 'or',\n",
       " 'other',\n",
       " 'our',\n",
       " 'ours',\n",
       " 'ourselves',\n",
       " 'out',\n",
       " 'over',\n",
       " 'own',\n",
       " 're',\n",
       " 's',\n",
       " 'same',\n",
       " 'shan',\n",
       " \"shan't\",\n",
       " 'she',\n",
       " \"she's\",\n",
       " 'should',\n",
       " \"should've\",\n",
       " 'shouldn',\n",
       " \"shouldn't\",\n",
       " 'so',\n",
       " 'some',\n",
       " 'such',\n",
       " 't',\n",
       " 'than',\n",
       " 'that',\n",
       " \"that'll\",\n",
       " 'the',\n",
       " 'their',\n",
       " 'theirs',\n",
       " 'them',\n",
       " 'themselves',\n",
       " 'then',\n",
       " 'there',\n",
       " 'these',\n",
       " 'they',\n",
       " 'this',\n",
       " 'those',\n",
       " 'through',\n",
       " 'to',\n",
       " 'too',\n",
       " 'under',\n",
       " 'until',\n",
       " 'up',\n",
       " 've',\n",
       " 'very',\n",
       " 'was',\n",
       " 'wasn',\n",
       " \"wasn't\",\n",
       " 'we',\n",
       " 'were',\n",
       " 'weren',\n",
       " \"weren't\",\n",
       " 'what',\n",
       " 'when',\n",
       " 'where',\n",
       " 'which',\n",
       " 'while',\n",
       " 'who',\n",
       " 'whom',\n",
       " 'why',\n",
       " 'will',\n",
       " 'with',\n",
       " 'won',\n",
       " \"won't\",\n",
       " 'wouldn',\n",
       " \"wouldn't\",\n",
       " 'y',\n",
       " 'you',\n",
       " \"you'd\",\n",
       " \"you'll\",\n",
       " \"you're\",\n",
       " \"you've\",\n",
       " 'your',\n",
       " 'yours',\n",
       " 'yourself',\n",
       " 'yourselves'}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stop_words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa6cafc7-2d0f-44eb-a3f8-387b271186b9",
   "metadata": {},
   "source": [
    "Then we can filter those English stop words from the tokens in course1:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "da73aa06-5f0e-47fb-8611-cccc773fee76",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['this',\n",
       " 'is',\n",
       " 'an',\n",
       " 'introduction',\n",
       " 'data',\n",
       " 'science',\n",
       " 'course',\n",
       " 'which',\n",
       " 'introduces',\n",
       " 'data',\n",
       " 'science',\n",
       " 'to',\n",
       " 'beginners']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Tokens in course 1\n",
    "tokenized_courses[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0f284775-3836-4e5f-8c09-fcdb18302543",
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_tokens = [w for w in tokenized_courses[0] if not w.lower() in stop_words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3c31765f-05e9-47b9-97d2-75ce1026a2db",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['introduction',\n",
       " 'data',\n",
       " 'science',\n",
       " 'course',\n",
       " 'introduces',\n",
       " 'data',\n",
       " 'science',\n",
       " 'beginners']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processed_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "840d09cd-a996-4615-bfef-d65b7442af42",
   "metadata": {},
   "source": [
    "You can see the number of tokens for ```course1``` has been reduced.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c89f467-4458-4fb5-9d35-4d7cd69d7836",
   "metadata": {},
   "source": [
    "Another common way is to only keep nouns in the text. We can use the `nltk.pos_tag()` method to analyze the part of speech (POS) and annotate each word.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "fc0d0a6f-f543-42f6-a77f-22cd9bd9fed0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('this', 'DT'),\n",
       " ('is', 'VBZ'),\n",
       " ('an', 'DT'),\n",
       " ('introduction', 'NN'),\n",
       " ('data', 'NNS'),\n",
       " ('science', 'NN'),\n",
       " ('course', 'NN'),\n",
       " ('which', 'WDT'),\n",
       " ('introduces', 'VBZ'),\n",
       " ('data', 'NNS'),\n",
       " ('science', 'NN'),\n",
       " ('to', 'TO'),\n",
       " ('beginners', 'NNS')]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tags = nltk.pos_tag(tokenized_courses[0])\n",
    "tags"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8981392b-9dc1-445a-9fe9-782bdceefa17",
   "metadata": {},
   "source": [
    "As we can see [`introduction`, `data`, `science`, `course`, `beginners`] are all of the nouns and we may keep them in the BoW feature vector.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea136fa2-cff6-4476-8374-7e9dadbdf73b",
   "metadata": {},
   "source": [
    "### TASK: Extract BoW features for course textual content and build a dataset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "095447bc-e1c1-493c-9362-657d638206dc",
   "metadata": {},
   "source": [
    "By now you have learned what a BoW feature is, so let's start extracting BoW features from some real course textual content.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "dac2f543-7559-430e-a8c3-02fd718f6e6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "course_url = \"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBM-ML321EN-SkillsNetwork/labs/datasets/course_processed.csv\"\n",
    "course_content_df = pd.read_csv(course_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9a242754-0467-4700-928f-e0d682a42b6f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "COURSE_ID                                               ML0201EN\n",
       "TITLE          robots are coming  build iot apps with watson ...\n",
       "DESCRIPTION    have fun with iot and learn along the way  if ...\n",
       "Name: 0, dtype: object"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "course_content_df.iloc[0, :]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "500b3ee5-b212-4a83-b212-9127097e3fd2",
   "metadata": {},
   "source": [
    "The course content dataset has three columns `COURSE_ID`, `TITLE`, and `DESCRIPTION`. `TITLE` and `DESCRIPTION` are all text upon which we want to extract BoW features. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd189679-0e1c-4c90-a44e-369a4c245335",
   "metadata": {},
   "source": [
    "Let's join those two text columns together.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "06e0ffd9-2987-4ffc-9f22-6e132794ae0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge TITLE and DESCRIPTION title\n",
    "course_content_df['course_texts'] = course_content_df[['TITLE', 'DESCRIPTION']].agg(' '.join, axis=1)\n",
    "course_content_df = course_content_df.reset_index()\n",
    "course_content_df['index'] = course_content_df.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0321fccd-904f-4b41-a368-bb732e828ae3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "index                                                           0\n",
       "COURSE_ID                                                ML0201EN\n",
       "TITLE           robots are coming  build iot apps with watson ...\n",
       "DESCRIPTION     have fun with iot and learn along the way  if ...\n",
       "course_texts    robots are coming  build iot apps with watson ...\n",
       "Name: 0, dtype: object"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "course_content_df.iloc[0, :]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25ef1252-1211-4c1a-96dc-0b078087c7e4",
   "metadata": {},
   "source": [
    "and we have prepared a `tokenize_course()` method for you to tokenize the course content:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "7c3a355c-b8bf-466e-8d27-d6658040bf7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_course(course, keep_only_nouns=True):\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    word_tokens = word_tokenize(course)\n",
    "    # Remove English stop words and numbers\n",
    "    word_tokens = [w for w in word_tokens if (not w.lower() in stop_words) and (not w.isnumeric())]\n",
    "    # Only keep nouns \n",
    "    if keep_only_nouns:\n",
    "        filter_list = ['WDT', 'WP', 'WRB', 'FW', 'IN', 'JJR', 'JJS', 'MD', 'PDT', 'POS', 'PRP', 'RB', 'RBR', 'RBS',\n",
    "                       'RP']\n",
    "        tags = nltk.pos_tag(word_tokens)\n",
    "        word_tokens = [word for word, pos in tags if pos not in filter_list]\n",
    "\n",
    "    return word_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c858bb0f-8bc6-4e52-aea8-f733b2b3890a",
   "metadata": {},
   "source": [
    "Let's try it on the first course.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "4d53a6ee-a980-46c9-acb4-6c28871d1e51",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'robots are coming  build iot apps with watson  swift  and node red have fun with iot and learn along the way  if you re a swift developer and want to learn more about iot and watson ai services in the cloud  raspberry pi   and node red  you ve found the right place  you ll build iot apps to read temperature data  take pictures with a raspcam  use ai to recognize the objects in those pictures  and program an irobot create 2 robot  '"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a_course = course_content_df.iloc[0, :]['course_texts']\n",
    "a_course"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'robots are coming  build iot apps with watson  swift  and node red have fun with iot and learn along the way  if you re a swift developer and want to learn more about iot and watson ai services in the cloud  raspberry pi   and node red  you ve found the right place  you ll build iot apps to read temperature data  take pictures with a raspcam  use ai to recognize the objects in those pictures  and program an irobot create 2 robot  '"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "course_content_df.iloc[0, :]['course_texts']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "424804e4-b2da-406f-9212-e9f1253b7073",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['robots',\n",
       " 'coming',\n",
       " 'build',\n",
       " 'iot',\n",
       " 'apps',\n",
       " 'watson',\n",
       " 'swift',\n",
       " 'red',\n",
       " 'fun',\n",
       " 'iot',\n",
       " 'learn',\n",
       " 'way',\n",
       " 'swift',\n",
       " 'developer',\n",
       " 'want',\n",
       " 'learn',\n",
       " 'iot',\n",
       " 'watson',\n",
       " 'ai',\n",
       " 'services',\n",
       " 'cloud',\n",
       " 'raspberry',\n",
       " 'pi',\n",
       " 'node',\n",
       " 'red',\n",
       " 'found',\n",
       " 'place',\n",
       " 'build',\n",
       " 'iot',\n",
       " 'apps',\n",
       " 'read',\n",
       " 'temperature',\n",
       " 'data',\n",
       " 'take',\n",
       " 'pictures',\n",
       " 'raspcam',\n",
       " 'use',\n",
       " 'ai',\n",
       " 'recognize',\n",
       " 'objects',\n",
       " 'pictures',\n",
       " 'program',\n",
       " 'irobot',\n",
       " 'create',\n",
       " 'robot']"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenize_course(a_course)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39907555-523c-46e8-ac87-9d48ed4e63af",
   "metadata": {},
   "source": [
    "Next, you will need to write some code snippets to generate the BoW features for each course. Let's start by tokenzing all courses in the `courses_df`:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "767bcfdc-b728-4540-ba91-8f52caa916ce",
   "metadata": {},
   "source": [
    "_TODO: Use provided tokenize_course() method to tokenize all courses in courses_df['course_texts']._\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = course_content_df['course_texts'].to_string()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "8f8d4842-97d0-4162-8958-51801bc3f080",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['robots',\n",
       " 'coming',\n",
       " 'build',\n",
       " 'iot',\n",
       " 'apps',\n",
       " 'watson',\n",
       " '...',\n",
       " 'accelerating',\n",
       " 'deep',\n",
       " 'learning',\n",
       " 'gpu',\n",
       " 'training',\n",
       " 'c',\n",
       " '...',\n",
       " 'consuming',\n",
       " 'restful',\n",
       " 'services',\n",
       " 'using',\n",
       " 'reactive',\n",
       " '...',\n",
       " 'analyzing',\n",
       " 'big',\n",
       " 'data',\n",
       " 'r',\n",
       " 'using',\n",
       " 'apache',\n",
       " 'spark',\n",
       " 'apa',\n",
       " '...',\n",
       " 'containerizing',\n",
       " 'packaging',\n",
       " 'running',\n",
       " 'sprin',\n",
       " '...',\n",
       " 'cloud',\n",
       " 'native',\n",
       " 'security',\n",
       " 'conference',\n",
       " 'data',\n",
       " 'securi',\n",
       " '...',\n",
       " 'data',\n",
       " 'science',\n",
       " 'bootcamp',\n",
       " 'r',\n",
       " 'university',\n",
       " 'pr',\n",
       " '...',\n",
       " 'learn',\n",
       " 'use',\n",
       " 'docker',\n",
       " 'containers',\n",
       " 'iterati',\n",
       " '...',\n",
       " 'scorm',\n",
       " 'test',\n",
       " 'scron',\n",
       " 'test',\n",
       " 'course',\n",
       " 'create',\n",
       " 'first',\n",
       " 'mongodb',\n",
       " 'database',\n",
       " 'gui',\n",
       " '...',\n",
       " 'testing',\n",
       " 'microservices',\n",
       " 'arquillian',\n",
       " 'mana',\n",
       " '...',\n",
       " 'cloud',\n",
       " 'pak',\n",
       " 'integration',\n",
       " 'essentials',\n",
       " '...',\n",
       " 'watson',\n",
       " 'analytics',\n",
       " 'social',\n",
       " 'media',\n",
       " 'watson',\n",
       " 'analy',\n",
       " '...',\n",
       " 'data',\n",
       " 'science',\n",
       " 'bootcamp',\n",
       " 'python',\n",
       " 'universi',\n",
       " '...',\n",
       " 'create',\n",
       " 'cryptocurrency',\n",
       " 'trading',\n",
       " 'algorithm',\n",
       " 'p',\n",
       " '...',\n",
       " 'data',\n",
       " 'ai',\n",
       " 'essentials',\n",
       " 'data',\n",
       " 'ai',\n",
       " 'essentials',\n",
       " 'co',\n",
       " '...',\n",
       " 'securing',\n",
       " 'java',\n",
       " 'microservices',\n",
       " 'eclipse',\n",
       " 'micro',\n",
       " '...',\n",
       " 'enabling',\n",
       " 'distributed',\n",
       " 'tracing',\n",
       " 'microservices',\n",
       " '...',\n",
       " 'sql',\n",
       " 'access',\n",
       " 'hadoop',\n",
       " 'big',\n",
       " 'sql',\n",
       " 'another',\n",
       " 'tool',\n",
       " '...',\n",
       " 'hybrid',\n",
       " 'cloud',\n",
       " 'conference',\n",
       " 'ai',\n",
       " 'pipelines',\n",
       " 'lab',\n",
       " 'hybr',\n",
       " '...',\n",
       " 'dataops',\n",
       " 'methodology',\n",
       " 'data',\n",
       " 'ops',\n",
       " 'course',\n",
       " 'data',\n",
       " 'ai',\n",
       " 'jumpstart',\n",
       " 'journey',\n",
       " 'introduce',\n",
       " 'yo',\n",
       " '...',\n",
       " 'introduction',\n",
       " 'open',\n",
       " 'source',\n",
       " 'course',\n",
       " 'introd',\n",
       " '...',\n",
       " 'end',\n",
       " 'end',\n",
       " 'data',\n",
       " 'science',\n",
       " 'cloudpak',\n",
       " 'data',\n",
       " 'e',\n",
       " '...',\n",
       " 'ai',\n",
       " 'everyone',\n",
       " 'master',\n",
       " 'basics',\n",
       " 'learn',\n",
       " '...',\n",
       " 'serverless',\n",
       " 'computing',\n",
       " 'using',\n",
       " 'cloud',\n",
       " 'functions',\n",
       " '...',\n",
       " 'predicting',\n",
       " 'customer',\n",
       " 'satisfaction',\n",
       " 'predict',\n",
       " 'custo',\n",
       " '...',\n",
       " 'hybrid',\n",
       " 'cloud',\n",
       " 'conference',\n",
       " 'serverless',\n",
       " 'lab',\n",
       " 'hybrid',\n",
       " '...',\n",
       " 'validating',\n",
       " 'constraints',\n",
       " 'javabeans',\n",
       " 'java',\n",
       " '...',\n",
       " 'getting',\n",
       " 'started',\n",
       " 'data',\n",
       " 'apache',\n",
       " 'spark',\n",
       " '...',\n",
       " 'predicting',\n",
       " 'financial',\n",
       " 'performance',\n",
       " 'company',\n",
       " '...',\n",
       " 'db2',\n",
       " 'fundamentals',\n",
       " 'db2',\n",
       " 'database',\n",
       " 'course',\n",
       " 'using',\n",
       " 'clustering',\n",
       " 'methods',\n",
       " 'investment',\n",
       " 'portfo',\n",
       " '...',\n",
       " 'php',\n",
       " 'web',\n",
       " 'application',\n",
       " 'lamp',\n",
       " 'stack',\n",
       " 'tutor',\n",
       " '...',\n",
       " 'fundamentals',\n",
       " 'javascript',\n",
       " 'rock',\n",
       " 'paper',\n",
       " '...',\n",
       " 'using',\n",
       " 'r',\n",
       " 'databases',\n",
       " 'intent',\n",
       " 'cour',\n",
       " '...',\n",
       " 'using',\n",
       " 'r',\n",
       " 'databases',\n",
       " 'using',\n",
       " 'r',\n",
       " 'databases',\n",
       " '...',\n",
       " 'spark',\n",
       " 'fundamentals',\n",
       " 'ii',\n",
       " 'building',\n",
       " 'foundat',\n",
       " '...',\n",
       " 'data',\n",
       " 'science',\n",
       " 'insurance',\n",
       " 'basic',\n",
       " 'statistical',\n",
       " '...',\n",
       " 'end',\n",
       " 'end',\n",
       " 'security',\n",
       " 'cloud',\n",
       " 'applicati',\n",
       " '...',\n",
       " 'spark',\n",
       " 'overview',\n",
       " 'scala',\n",
       " 'analytics',\n",
       " 'â€š',\n",
       " 'spar',\n",
       " '...',\n",
       " 'testing',\n",
       " 'microprofile',\n",
       " 'jakarta',\n",
       " 'ee',\n",
       " 'applicati',\n",
       " '...',\n",
       " 'r',\n",
       " 'introduction',\n",
       " 'r',\n",
       " 'mast',\n",
       " '...',\n",
       " 'text',\n",
       " 'analytics',\n",
       " 'analyze',\n",
       " 'text',\n",
       " 'data',\n",
       " 'using',\n",
       " 'ana',\n",
       " '...',\n",
       " 'build',\n",
       " 'watson',\n",
       " 'ai',\n",
       " 'swift',\n",
       " 'apis',\n",
       " 'make',\n",
       " '...',\n",
       " 'data',\n",
       " 'science',\n",
       " 'bootcamp',\n",
       " 'multi',\n",
       " 'day',\n",
       " 'intensive',\n",
       " '...',\n",
       " 'insurance',\n",
       " 'risk',\n",
       " 'assessment',\n",
       " 'montecarlo',\n",
       " 'meth',\n",
       " '...',\n",
       " 'statistics',\n",
       " 'statistics',\n",
       " 'courses',\n",
       " 'teaching',\n",
       " 'bas',\n",
       " '...',\n",
       " 'statistics',\n",
       " 'statistics',\n",
       " 'courses',\n",
       " 'teaching',\n",
       " 'adv',\n",
       " '...',\n",
       " 'build',\n",
       " 'swift',\n",
       " 'mobile',\n",
       " 'apps',\n",
       " 'watson',\n",
       " 'ai',\n",
       " 'service',\n",
       " '...',\n",
       " 'text',\n",
       " 'analysis',\n",
       " 'analyze',\n",
       " 'text',\n",
       " 'data',\n",
       " 'using',\n",
       " 'various',\n",
       " '...',\n",
       " 'introduction',\n",
       " 'machine',\n",
       " 'learning',\n",
       " 'sound',\n",
       " 'ge',\n",
       " '...',\n",
       " 'using',\n",
       " 'hbase',\n",
       " 'real',\n",
       " 'time',\n",
       " 'access',\n",
       " 'big',\n",
       " '...',\n",
       " 'watson',\n",
       " 'analytics',\n",
       " 'welcome',\n",
       " 'watson',\n",
       " 'analyti',\n",
       " '...',\n",
       " 'insurance',\n",
       " 'business',\n",
       " 'modelling',\n",
       " 'basic',\n",
       " 'actuari',\n",
       " '...',\n",
       " 'accessing',\n",
       " 'hadoop',\n",
       " 'data',\n",
       " 'using',\n",
       " 'hive',\n",
       " 'hive',\n",
       " 'dat',\n",
       " '...',\n",
       " 'basics',\n",
       " 'istio',\n",
       " 'ibm',\n",
       " 'cloud',\n",
       " 'kuberne',\n",
       " '...',\n",
       " 'accelerating',\n",
       " 'deep',\n",
       " 'learning',\n",
       " 'gpu',\n",
       " 'majority',\n",
       " '...',\n",
       " 'text',\n",
       " 'analytics',\n",
       " 'analysis',\n",
       " 'emails',\n",
       " 'blo',\n",
       " '...',\n",
       " 'text',\n",
       " 'analytics',\n",
       " 'scale',\n",
       " 'continuation',\n",
       " 'text',\n",
       " '...',\n",
       " 'data',\n",
       " 'science',\n",
       " 'bootcamp',\n",
       " 'python',\n",
       " 'data',\n",
       " 'science',\n",
       " '...',\n",
       " 'machine',\n",
       " 'learning',\n",
       " 'apache',\n",
       " 'systemml',\n",
       " 'apache',\n",
       " '...',\n",
       " 'action',\n",
       " 'classification',\n",
       " 'task',\n",
       " 'based',\n",
       " 'internet',\n",
       " 'f',\n",
       " '...',\n",
       " 'container',\n",
       " 'kubernetes',\n",
       " 'essentials',\n",
       " 'ibm',\n",
       " 'clo',\n",
       " '...',\n",
       " 'data',\n",
       " 'science',\n",
       " 'health',\n",
       " 'care',\n",
       " 'advanced',\n",
       " 'prognost',\n",
       " '...',\n",
       " 'advanced',\n",
       " 'machine',\n",
       " 'deep',\n",
       " 'learning',\n",
       " 'spam',\n",
       " 'clas',\n",
       " '...',\n",
       " 'visual',\n",
       " 'data',\n",
       " 'analysis',\n",
       " 'banking',\n",
       " 'learn',\n",
       " 'maste',\n",
       " '...',\n",
       " 'secure',\n",
       " 'analysis',\n",
       " 'credit',\n",
       " 'card',\n",
       " 'dataset',\n",
       " 'learn',\n",
       " '...',\n",
       " 'data',\n",
       " 'science',\n",
       " 'health',\n",
       " 'care',\n",
       " 'advanced',\n",
       " 'machine',\n",
       " '...',\n",
       " 'network',\n",
       " 'traffic',\n",
       " 'anomaly',\n",
       " 'detection',\n",
       " 'intrusion',\n",
       " '...',\n",
       " 'getting',\n",
       " 'started',\n",
       " 'node',\n",
       " 'js',\n",
       " 'heard',\n",
       " '...',\n",
       " 'getting',\n",
       " 'started',\n",
       " 'mysql',\n",
       " 'command',\n",
       " 'line',\n",
       " 'thi',\n",
       " '...',\n",
       " 'introduction',\n",
       " 'ibm',\n",
       " 'cloud',\n",
       " 'satellite',\n",
       " 'short',\n",
       " '...',\n",
       " 'introduction',\n",
       " 'quantum',\n",
       " 'computing',\n",
       " 'course',\n",
       " '...',\n",
       " 'launch',\n",
       " 'ai',\n",
       " 'hotdog',\n",
       " 'detector',\n",
       " 'serverless',\n",
       " 'p',\n",
       " '...',\n",
       " 'exploratory',\n",
       " 'data',\n",
       " 'analysis',\n",
       " 'eda',\n",
       " 'pandas',\n",
       " '...',\n",
       " 'text',\n",
       " 'analytics',\n",
       " 'social',\n",
       " 'media',\n",
       " 'news',\n",
       " '...',\n",
       " 'create',\n",
       " 'tables',\n",
       " 'data',\n",
       " 'mysql',\n",
       " 'using',\n",
       " 'php',\n",
       " '...',\n",
       " 'relational',\n",
       " 'model',\n",
       " 'concepts',\n",
       " 'project',\n",
       " '...',\n",
       " 'predictive',\n",
       " 'modeling',\n",
       " 'fundamentals',\n",
       " 'predict',\n",
       " '...',\n",
       " 'build',\n",
       " 'iot',\n",
       " 'blockchain',\n",
       " 'network',\n",
       " 'supply',\n",
       " 'c',\n",
       " '...',\n",
       " 'blockchain',\n",
       " 'essentials',\n",
       " 'understand',\n",
       " 'blockchain',\n",
       " 'te',\n",
       " '...',\n",
       " 'getting',\n",
       " 'started',\n",
       " 'postgresql',\n",
       " 'command',\n",
       " 'line',\n",
       " '...',\n",
       " 'ibm',\n",
       " 'blockchain',\n",
       " 'foundation',\n",
       " 'developer',\n",
       " 'dive',\n",
       " 'deepe',\n",
       " '...',\n",
       " 'keys',\n",
       " 'constraints',\n",
       " 'mysql',\n",
       " 'project',\n",
       " '...',\n",
       " 'exploring',\n",
       " 'spark',\n",
       " 'graphx',\n",
       " 'spark',\n",
       " 'provides',\n",
       " 'grap',\n",
       " '...',\n",
       " 'data',\n",
       " 'science',\n",
       " 'health',\n",
       " 'care',\n",
       " 'basic',\n",
       " 'statistical',\n",
       " '...',\n",
       " 'create',\n",
       " 'tables',\n",
       " 'data',\n",
       " 'postgresql',\n",
       " 'usin',\n",
       " '...',\n",
       " 'monitoring',\n",
       " 'metrics',\n",
       " 'java',\n",
       " 'microservices',\n",
       " 'u',\n",
       " '...',\n",
       " 'configuring',\n",
       " 'microservices',\n",
       " 'running',\n",
       " 'kubernete',\n",
       " '...',\n",
       " 'game',\n",
       " 'playing',\n",
       " 'ai',\n",
       " 'swift',\n",
       " 'tensorflow',\n",
       " 's4t',\n",
       " '...',\n",
       " 'enabling',\n",
       " 'cross',\n",
       " 'origin',\n",
       " 'resource',\n",
       " 'sharing',\n",
       " 'cors',\n",
       " '...',\n",
       " 'getting',\n",
       " 'started',\n",
       " 'db2',\n",
       " 'cloud',\n",
       " 'proj',\n",
       " '...',\n",
       " 'apache',\n",
       " 'pig',\n",
       " 'pig',\n",
       " 'developed',\n",
       " '...',\n",
       " 'developing',\n",
       " 'distributed',\n",
       " 'applications',\n",
       " 'using',\n",
       " 'zook',\n",
       " '...',\n",
       " 'controlling',\n",
       " 'hadoop',\n",
       " 'jobs',\n",
       " 'using',\n",
       " 'oozie',\n",
       " 'apach',\n",
       " '...',\n",
       " 'moving',\n",
       " 'data',\n",
       " 'hadoop',\n",
       " 'course',\n",
       " 'describes',\n",
       " '...',\n",
       " 'mapreduce',\n",
       " 'yarn',\n",
       " 'string',\n",
       " 'unders',\n",
       " '...',\n",
       " 'deploy',\n",
       " 'web',\n",
       " 'server',\n",
       " 'using',\n",
       " 'python',\n",
       " 'ibm',\n",
       " 'cloud',\n",
       " '...',\n",
       " 'acknowledging',\n",
       " 'messages',\n",
       " 'using',\n",
       " 'microprofile',\n",
       " 'reac',\n",
       " '...',\n",
       " 'build',\n",
       " 'personal',\n",
       " 'movie',\n",
       " 'recommender',\n",
       " 'django',\n",
       " '...',\n",
       " 'managing',\n",
       " 'injecting',\n",
       " 'dependencies',\n",
       " 'java',\n",
       " '...',\n",
       " 'spark',\n",
       " 'fundamentals',\n",
       " 'ignite',\n",
       " 'interest',\n",
       " '...',\n",
       " 'deploying',\n",
       " 'microservices',\n",
       " 'kubernetes',\n",
       " 'deploy',\n",
       " '...',\n",
       " 'getting',\n",
       " 'started',\n",
       " 'open',\n",
       " 'liberty',\n",
       " 'learn',\n",
       " '...',\n",
       " 'consuming',\n",
       " 'restful',\n",
       " 'java',\n",
       " 'microservices',\n",
       " 'asynchron',\n",
       " '...',\n",
       " 'statistics',\n",
       " 'take',\n",
       " 'course',\n",
       " '...',\n",
       " 'analyzing',\n",
       " 'big',\n",
       " 'data',\n",
       " 'spreadsheet',\n",
       " 'ui',\n",
       " '...',\n",
       " 'openrefine',\n",
       " 'introduction',\n",
       " 'course',\n",
       " '...',\n",
       " 'solr',\n",
       " 'course',\n",
       " 'introduce',\n",
       " 'solr',\n",
       " 'deep',\n",
       " 'learning',\n",
       " 'tensorflow',\n",
       " 'majority',\n",
       " 'data',\n",
       " '...',\n",
       " 'building',\n",
       " 'fault',\n",
       " 'tolerant',\n",
       " 'microservices',\n",
       " '...',\n",
       " 'consuming',\n",
       " 'restful',\n",
       " 'java',\n",
       " 'microservices',\n",
       " 'temp',\n",
       " '...',\n",
       " 'build',\n",
       " 'chatbots',\n",
       " 'watson',\n",
       " 'assistant',\n",
       " 'c',\n",
       " '...',\n",
       " 'red',\n",
       " 'basics',\n",
       " 'bots',\n",
       " 'create',\n",
       " 'cognitive',\n",
       " 'web',\n",
       " '...',\n",
       " 'normalization',\n",
       " 'keys',\n",
       " 'constraints',\n",
       " 'relation',\n",
       " '...',\n",
       " 'ibm',\n",
       " 'cloud',\n",
       " 'essentials',\n",
       " 'ibm',\n",
       " 'cloud',\n",
       " 'name',\n",
       " '...',\n",
       " 'getting',\n",
       " 'started',\n",
       " 'microservices',\n",
       " 'istio',\n",
       " '...',\n",
       " 'playing',\n",
       " 'tictactoe',\n",
       " 'reinforcement',\n",
       " 'learning',\n",
       " '...',\n",
       " 'deploying',\n",
       " 'microservice',\n",
       " 'openshift',\n",
       " 'using',\n",
       " '...',\n",
       " 'data',\n",
       " 'analysis',\n",
       " 'demos',\n",
       " 'course',\n",
       " 'provi',\n",
       " '...',\n",
       " 'data',\n",
       " 'journalism',\n",
       " 'first',\n",
       " 'steps',\n",
       " 'skills',\n",
       " 'tools',\n",
       " '...',\n",
       " 'mathematical',\n",
       " 'optimization',\n",
       " 'business',\n",
       " 'problem',\n",
       " '...',\n",
       " 'db2',\n",
       " 'academic',\n",
       " 'training',\n",
       " 'db2',\n",
       " 'academic',\n",
       " 'training',\n",
       " 'db2',\n",
       " 'fundamentals',\n",
       " 'ii',\n",
       " 'db2',\n",
       " 'fundamentals',\n",
       " 'ii',\n",
       " 'scalable',\n",
       " 'web',\n",
       " 'applications',\n",
       " 'kubernetes',\n",
       " '...',\n",
       " 'data',\n",
       " 'analysis',\n",
       " 'using',\n",
       " 'r',\n",
       " 'data',\n",
       " 'analysis',\n",
       " 'using',\n",
       " '...',\n",
       " 'nosql',\n",
       " 'dbaas',\n",
       " 'nosql',\n",
       " 'course',\n",
       " 'w',\n",
       " '...',\n",
       " 'kubernetes',\n",
       " 'operators',\n",
       " 'advanced',\n",
       " 'course',\n",
       " 'cove',\n",
       " '...',\n",
       " 'project',\n",
       " 'deploy',\n",
       " 'serverless',\n",
       " 'app',\n",
       " 'image',\n",
       " '...',\n",
       " 'data',\n",
       " 'science',\n",
       " 'career',\n",
       " 'talks',\n",
       " 'data',\n",
       " 'science',\n",
       " 'career',\n",
       " '...',\n",
       " 'data',\n",
       " 'science',\n",
       " 'open',\n",
       " 'data',\n",
       " 'data',\n",
       " 'science',\n",
       " '...',\n",
       " 'data',\n",
       " 'science',\n",
       " 'bootcamp',\n",
       " 'python',\n",
       " 'universi',\n",
       " '...',\n",
       " 'bitcoin',\n",
       " 'greetings',\n",
       " 'welcome',\n",
       " 'intro',\n",
       " '...',\n",
       " 'data',\n",
       " 'science',\n",
       " 'hands',\n",
       " 'open',\n",
       " 'source',\n",
       " 'tools',\n",
       " 'w',\n",
       " '...',\n",
       " 'data',\n",
       " 'science',\n",
       " 'methodology',\n",
       " 'grab',\n",
       " 'lab',\n",
       " 'coat',\n",
       " '...',\n",
       " 'creating',\n",
       " 'asynchronous',\n",
       " 'java',\n",
       " 'microservices',\n",
       " 'using',\n",
       " '...',\n",
       " 'build',\n",
       " 'smart',\n",
       " 'search',\n",
       " 'form',\n",
       " 'algolia',\n",
       " 'great',\n",
       " '...',\n",
       " 'documenting',\n",
       " 'restful',\n",
       " 'apis',\n",
       " 'using',\n",
       " 'microprofile',\n",
       " 'op',\n",
       " '...',\n",
       " 'reactive',\n",
       " 'architecture',\n",
       " 'distributed',\n",
       " 'messaging',\n",
       " 'p',\n",
       " '...',\n",
       " 'data',\n",
       " 'science',\n",
       " 'agriculture',\n",
       " 'land',\n",
       " 'use',\n",
       " 'classifi',\n",
       " '...',\n",
       " 'reactive',\n",
       " 'architecture',\n",
       " 'cqrs',\n",
       " 'event',\n",
       " 'sourcing',\n",
       " 'r',\n",
       " '...',\n",
       " 'deep',\n",
       " 'learning',\n",
       " 'deep',\n",
       " 'learning',\n",
       " 'reactive',\n",
       " 'architecture',\n",
       " 'reactive',\n",
       " 'microservices',\n",
       " '...',\n",
       " 'reactive',\n",
       " 'architecture',\n",
       " 'domain',\n",
       " 'driven',\n",
       " 'design',\n",
       " '...',\n",
       " 'building',\n",
       " 'robots',\n",
       " 'tjbot',\n",
       " 'learn',\n",
       " 'progra',\n",
       " '...',\n",
       " 'building',\n",
       " 'cloud',\n",
       " 'native',\n",
       " 'multicloud',\n",
       " 'applicati',\n",
       " '...',\n",
       " 'big',\n",
       " 'data',\n",
       " 'big',\n",
       " 'big',\n",
       " 'big',\n",
       " '...',\n",
       " 'modernizing',\n",
       " 'java',\n",
       " 'ee',\n",
       " 'applications',\n",
       " 'learn',\n",
       " '...',\n",
       " 'data',\n",
       " 'science',\n",
       " 'agriculture',\n",
       " 'prognostication',\n",
       " 'u',\n",
       " '...',\n",
       " 'docker',\n",
       " 'essentials',\n",
       " 'developer',\n",
       " 'introduction',\n",
       " 'le',\n",
       " '...',\n",
       " 'accelerating',\n",
       " 'deep',\n",
       " 'learning',\n",
       " 'gpus',\n",
       " 'training',\n",
       " '...',\n",
       " 'reactive',\n",
       " 'architecture',\n",
       " 'building',\n",
       " 'scalable',\n",
       " 'syste',\n",
       " '...',\n",
       " 'build',\n",
       " 'chatbots',\n",
       " 'learn',\n",
       " 'build',\n",
       " 'cha',\n",
       " '...',\n",
       " 'data',\n",
       " 'science',\n",
       " 'agriculture',\n",
       " 'basic',\n",
       " 'statistical',\n",
       " '...',\n",
       " 'machine',\n",
       " 'learning',\n",
       " 'python',\n",
       " 'phrases',\n",
       " 'â€š',\n",
       " '...',\n",
       " 'build',\n",
       " 'chatbot',\n",
       " 'build',\n",
       " 'chat',\n",
       " '...',\n",
       " 'machine',\n",
       " 'learning',\n",
       " 'dimensionality',\n",
       " 'reduction',\n",
       " '...',\n",
       " 'machine',\n",
       " 'learning',\n",
       " 'python',\n",
       " 'machine',\n",
       " 'learning',\n",
       " '...',\n",
       " 'consuming',\n",
       " 'restful',\n",
       " 'java',\n",
       " 'web',\n",
       " 'service',\n",
       " 'using',\n",
       " 'jso',\n",
       " '...',\n",
       " 'introduction',\n",
       " 'ibm',\n",
       " 'cloud',\n",
       " 'financial',\n",
       " 'ser',\n",
       " '...',\n",
       " 'data',\n",
       " 'analysis',\n",
       " 'python',\n",
       " 'course',\n",
       " 'w',\n",
       " '...',\n",
       " ...]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# WRITE YOUR CODE HERE\n",
    "tokenized = tokenize_course(x)\n",
    "tokenized"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afe9c3cd-d79b-42d2-8b53-ba13e12b9530",
   "metadata": {},
   "source": [
    "\n",
    "<details>\n",
    "    <summary>Click here for Hints</summary>\n",
    "\n",
    "Use `tokenize_course(text, True)` command to tokenize each text in `courses_df['course_texts']`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84f7ed6c-6690-4221-b2c1-4125aad7e1b1",
   "metadata": {},
   "source": [
    "Then we need to create a token dictionary `tokens_dict`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f499589-e120-4902-8107-1ce0eb3c8a88",
   "metadata": {},
   "source": [
    "_TODO: Use gensim.corpora.Dictionary(tokenized_courses) to create a token dictionary._\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['robots'],\n",
       " ['coming'],\n",
       " ['build'],\n",
       " ['iot'],\n",
       " ['apps'],\n",
       " ['watson'],\n",
       " ['...'],\n",
       " ['accelerating'],\n",
       " ['deep'],\n",
       " ['learning'],\n",
       " ['gpu'],\n",
       " ['training'],\n",
       " ['c'],\n",
       " ['...'],\n",
       " ['consuming'],\n",
       " ['restful'],\n",
       " ['services'],\n",
       " ['using'],\n",
       " ['reactive'],\n",
       " ['...'],\n",
       " ['analyzing'],\n",
       " ['big'],\n",
       " ['data'],\n",
       " ['r'],\n",
       " ['using'],\n",
       " ['apache'],\n",
       " ['spark'],\n",
       " ['apa'],\n",
       " ['...'],\n",
       " ['containerizing'],\n",
       " ['packaging'],\n",
       " ['running'],\n",
       " ['sprin'],\n",
       " ['...'],\n",
       " ['cloud'],\n",
       " ['native'],\n",
       " ['security'],\n",
       " ['conference'],\n",
       " ['data'],\n",
       " ['securi'],\n",
       " ['...'],\n",
       " ['data'],\n",
       " ['science'],\n",
       " ['bootcamp'],\n",
       " ['r'],\n",
       " ['university'],\n",
       " ['pr'],\n",
       " ['...'],\n",
       " ['learn'],\n",
       " ['use'],\n",
       " ['docker'],\n",
       " ['containers'],\n",
       " ['iterati'],\n",
       " ['...'],\n",
       " ['scorm'],\n",
       " ['test'],\n",
       " ['scron'],\n",
       " ['test'],\n",
       " ['course'],\n",
       " ['create'],\n",
       " ['first'],\n",
       " ['mongodb'],\n",
       " ['database'],\n",
       " ['gui'],\n",
       " ['...'],\n",
       " ['testing'],\n",
       " ['microservices'],\n",
       " ['arquillian'],\n",
       " ['mana'],\n",
       " ['...'],\n",
       " ['cloud'],\n",
       " ['pak'],\n",
       " ['integration'],\n",
       " ['essentials'],\n",
       " ['...'],\n",
       " ['watson'],\n",
       " ['analytics'],\n",
       " ['social'],\n",
       " ['media'],\n",
       " ['watson'],\n",
       " ['analy'],\n",
       " ['...'],\n",
       " ['data'],\n",
       " ['science'],\n",
       " ['bootcamp'],\n",
       " ['python'],\n",
       " ['universi'],\n",
       " ['...'],\n",
       " ['create'],\n",
       " ['cryptocurrency'],\n",
       " ['trading'],\n",
       " ['algorithm'],\n",
       " ['p'],\n",
       " ['...'],\n",
       " ['data'],\n",
       " ['ai'],\n",
       " ['essentials'],\n",
       " ['data'],\n",
       " ['ai'],\n",
       " ['essentials'],\n",
       " ['co'],\n",
       " ['...'],\n",
       " ['securing'],\n",
       " ['java'],\n",
       " ['microservices'],\n",
       " ['eclipse'],\n",
       " ['micro'],\n",
       " ['...'],\n",
       " ['enabling'],\n",
       " ['distributed'],\n",
       " ['tracing'],\n",
       " ['microservices'],\n",
       " ['...'],\n",
       " ['sql'],\n",
       " ['access'],\n",
       " ['hadoop'],\n",
       " ['big'],\n",
       " ['sql'],\n",
       " ['another'],\n",
       " ['tool'],\n",
       " ['...'],\n",
       " ['hybrid'],\n",
       " ['cloud'],\n",
       " ['conference'],\n",
       " ['ai'],\n",
       " ['pipelines'],\n",
       " ['lab'],\n",
       " ['hybr'],\n",
       " ['...'],\n",
       " ['dataops'],\n",
       " ['methodology'],\n",
       " ['data'],\n",
       " ['ops'],\n",
       " ['course'],\n",
       " ['data'],\n",
       " ['ai'],\n",
       " ['jumpstart'],\n",
       " ['journey'],\n",
       " ['introduce'],\n",
       " ['yo'],\n",
       " ['...'],\n",
       " ['introduction'],\n",
       " ['open'],\n",
       " ['source'],\n",
       " ['course'],\n",
       " ['introd'],\n",
       " ['...'],\n",
       " ['end'],\n",
       " ['end'],\n",
       " ['data'],\n",
       " ['science'],\n",
       " ['cloudpak'],\n",
       " ['data'],\n",
       " ['e'],\n",
       " ['...'],\n",
       " ['ai'],\n",
       " ['everyone'],\n",
       " ['master'],\n",
       " ['basics'],\n",
       " ['learn'],\n",
       " ['...'],\n",
       " ['serverless'],\n",
       " ['computing'],\n",
       " ['using'],\n",
       " ['cloud'],\n",
       " ['functions'],\n",
       " ['...'],\n",
       " ['predicting'],\n",
       " ['customer'],\n",
       " ['satisfaction'],\n",
       " ['predict'],\n",
       " ['custo'],\n",
       " ['...'],\n",
       " ['hybrid'],\n",
       " ['cloud'],\n",
       " ['conference'],\n",
       " ['serverless'],\n",
       " ['lab'],\n",
       " ['hybrid'],\n",
       " ['...'],\n",
       " ['validating'],\n",
       " ['constraints'],\n",
       " ['javabeans'],\n",
       " ['java'],\n",
       " ['...'],\n",
       " ['getting'],\n",
       " ['started'],\n",
       " ['data'],\n",
       " ['apache'],\n",
       " ['spark'],\n",
       " ['...'],\n",
       " ['predicting'],\n",
       " ['financial'],\n",
       " ['performance'],\n",
       " ['company'],\n",
       " ['...'],\n",
       " ['db2'],\n",
       " ['fundamentals'],\n",
       " ['db2'],\n",
       " ['database'],\n",
       " ['course'],\n",
       " ['using'],\n",
       " ['clustering'],\n",
       " ['methods'],\n",
       " ['investment'],\n",
       " ['portfo'],\n",
       " ['...'],\n",
       " ['php'],\n",
       " ['web'],\n",
       " ['application'],\n",
       " ['lamp'],\n",
       " ['stack'],\n",
       " ['tutor'],\n",
       " ['...'],\n",
       " ['fundamentals'],\n",
       " ['javascript'],\n",
       " ['rock'],\n",
       " ['paper'],\n",
       " ['...'],\n",
       " ['using'],\n",
       " ['r'],\n",
       " ['databases'],\n",
       " ['intent'],\n",
       " ['cour'],\n",
       " ['...'],\n",
       " ['using'],\n",
       " ['r'],\n",
       " ['databases'],\n",
       " ['using'],\n",
       " ['r'],\n",
       " ['databases'],\n",
       " ['...'],\n",
       " ['spark'],\n",
       " ['fundamentals'],\n",
       " ['ii'],\n",
       " ['building'],\n",
       " ['foundat'],\n",
       " ['...'],\n",
       " ['data'],\n",
       " ['science'],\n",
       " ['insurance'],\n",
       " ['basic'],\n",
       " ['statistical'],\n",
       " ['...'],\n",
       " ['end'],\n",
       " ['end'],\n",
       " ['security'],\n",
       " ['cloud'],\n",
       " ['applicati'],\n",
       " ['...'],\n",
       " ['spark'],\n",
       " ['overview'],\n",
       " ['scala'],\n",
       " ['analytics'],\n",
       " ['â€š'],\n",
       " ['spar'],\n",
       " ['...'],\n",
       " ['testing'],\n",
       " ['microprofile'],\n",
       " ['jakarta'],\n",
       " ['ee'],\n",
       " ['applicati'],\n",
       " ['...'],\n",
       " ['r'],\n",
       " ['introduction'],\n",
       " ['r'],\n",
       " ['mast'],\n",
       " ['...'],\n",
       " ['text'],\n",
       " ['analytics'],\n",
       " ['analyze'],\n",
       " ['text'],\n",
       " ['data'],\n",
       " ['using'],\n",
       " ['ana'],\n",
       " ['...'],\n",
       " ['build'],\n",
       " ['watson'],\n",
       " ['ai'],\n",
       " ['swift'],\n",
       " ['apis'],\n",
       " ['make'],\n",
       " ['...'],\n",
       " ['data'],\n",
       " ['science'],\n",
       " ['bootcamp'],\n",
       " ['multi'],\n",
       " ['day'],\n",
       " ['intensive'],\n",
       " ['...'],\n",
       " ['insurance'],\n",
       " ['risk'],\n",
       " ['assessment'],\n",
       " ['montecarlo'],\n",
       " ['meth'],\n",
       " ['...'],\n",
       " ['statistics'],\n",
       " ['statistics'],\n",
       " ['courses'],\n",
       " ['teaching'],\n",
       " ['bas'],\n",
       " ['...'],\n",
       " ['statistics'],\n",
       " ['statistics'],\n",
       " ['courses'],\n",
       " ['teaching'],\n",
       " ['adv'],\n",
       " ['...'],\n",
       " ['build'],\n",
       " ['swift'],\n",
       " ['mobile'],\n",
       " ['apps'],\n",
       " ['watson'],\n",
       " ['ai'],\n",
       " ['service'],\n",
       " ['...'],\n",
       " ['text'],\n",
       " ['analysis'],\n",
       " ['analyze'],\n",
       " ['text'],\n",
       " ['data'],\n",
       " ['using'],\n",
       " ['various'],\n",
       " ['...'],\n",
       " ['introduction'],\n",
       " ['machine'],\n",
       " ['learning'],\n",
       " ['sound'],\n",
       " ['ge'],\n",
       " ['...'],\n",
       " ['using'],\n",
       " ['hbase'],\n",
       " ['real'],\n",
       " ['time'],\n",
       " ['access'],\n",
       " ['big'],\n",
       " ['...'],\n",
       " ['watson'],\n",
       " ['analytics'],\n",
       " ['welcome'],\n",
       " ['watson'],\n",
       " ['analyti'],\n",
       " ['...'],\n",
       " ['insurance'],\n",
       " ['business'],\n",
       " ['modelling'],\n",
       " ['basic'],\n",
       " ['actuari'],\n",
       " ['...'],\n",
       " ['accessing'],\n",
       " ['hadoop'],\n",
       " ['data'],\n",
       " ['using'],\n",
       " ['hive'],\n",
       " ['hive'],\n",
       " ['dat'],\n",
       " ['...'],\n",
       " ['basics'],\n",
       " ['istio'],\n",
       " ['ibm'],\n",
       " ['cloud'],\n",
       " ['kuberne'],\n",
       " ['...'],\n",
       " ['accelerating'],\n",
       " ['deep'],\n",
       " ['learning'],\n",
       " ['gpu'],\n",
       " ['majority'],\n",
       " ['...'],\n",
       " ['text'],\n",
       " ['analytics'],\n",
       " ['analysis'],\n",
       " ['emails'],\n",
       " ['blo'],\n",
       " ['...'],\n",
       " ['text'],\n",
       " ['analytics'],\n",
       " ['scale'],\n",
       " ['continuation'],\n",
       " ['text'],\n",
       " ['...'],\n",
       " ['data'],\n",
       " ['science'],\n",
       " ['bootcamp'],\n",
       " ['python'],\n",
       " ['data'],\n",
       " ['science'],\n",
       " ['...'],\n",
       " ['machine'],\n",
       " ['learning'],\n",
       " ['apache'],\n",
       " ['systemml'],\n",
       " ['apache'],\n",
       " ['...'],\n",
       " ['action'],\n",
       " ['classification'],\n",
       " ['task'],\n",
       " ['based'],\n",
       " ['internet'],\n",
       " ['f'],\n",
       " ['...'],\n",
       " ['container'],\n",
       " ['kubernetes'],\n",
       " ['essentials'],\n",
       " ['ibm'],\n",
       " ['clo'],\n",
       " ['...'],\n",
       " ['data'],\n",
       " ['science'],\n",
       " ['health'],\n",
       " ['care'],\n",
       " ['advanced'],\n",
       " ['prognost'],\n",
       " ['...'],\n",
       " ['advanced'],\n",
       " ['machine'],\n",
       " ['deep'],\n",
       " ['learning'],\n",
       " ['spam'],\n",
       " ['clas'],\n",
       " ['...'],\n",
       " ['visual'],\n",
       " ['data'],\n",
       " ['analysis'],\n",
       " ['banking'],\n",
       " ['learn'],\n",
       " ['maste'],\n",
       " ['...'],\n",
       " ['secure'],\n",
       " ['analysis'],\n",
       " ['credit'],\n",
       " ['card'],\n",
       " ['dataset'],\n",
       " ['learn'],\n",
       " ['...'],\n",
       " ['data'],\n",
       " ['science'],\n",
       " ['health'],\n",
       " ['care'],\n",
       " ['advanced'],\n",
       " ['machine'],\n",
       " ['...'],\n",
       " ['network'],\n",
       " ['traffic'],\n",
       " ['anomaly'],\n",
       " ['detection'],\n",
       " ['intrusion'],\n",
       " ['...'],\n",
       " ['getting'],\n",
       " ['started'],\n",
       " ['node'],\n",
       " ['js'],\n",
       " ['heard'],\n",
       " ['...'],\n",
       " ['getting'],\n",
       " ['started'],\n",
       " ['mysql'],\n",
       " ['command'],\n",
       " ['line'],\n",
       " ['thi'],\n",
       " ['...'],\n",
       " ['introduction'],\n",
       " ['ibm'],\n",
       " ['cloud'],\n",
       " ['satellite'],\n",
       " ['short'],\n",
       " ['...'],\n",
       " ['introduction'],\n",
       " ['quantum'],\n",
       " ['computing'],\n",
       " ['course'],\n",
       " ['...'],\n",
       " ['launch'],\n",
       " ['ai'],\n",
       " ['hotdog'],\n",
       " ['detector'],\n",
       " ['serverless'],\n",
       " ['p'],\n",
       " ['...'],\n",
       " ['exploratory'],\n",
       " ['data'],\n",
       " ['analysis'],\n",
       " ['eda'],\n",
       " ['pandas'],\n",
       " ['...'],\n",
       " ['text'],\n",
       " ['analytics'],\n",
       " ['social'],\n",
       " ['media'],\n",
       " ['news'],\n",
       " ['...'],\n",
       " ['create'],\n",
       " ['tables'],\n",
       " ['data'],\n",
       " ['mysql'],\n",
       " ['using'],\n",
       " ['php'],\n",
       " ['...'],\n",
       " ['relational'],\n",
       " ['model'],\n",
       " ['concepts'],\n",
       " ['project'],\n",
       " ['...'],\n",
       " ['predictive'],\n",
       " ['modeling'],\n",
       " ['fundamentals'],\n",
       " ['predict'],\n",
       " ['...'],\n",
       " ['build'],\n",
       " ['iot'],\n",
       " ['blockchain'],\n",
       " ['network'],\n",
       " ['supply'],\n",
       " ['c'],\n",
       " ['...'],\n",
       " ['blockchain'],\n",
       " ['essentials'],\n",
       " ['understand'],\n",
       " ['blockchain'],\n",
       " ['te'],\n",
       " ['...'],\n",
       " ['getting'],\n",
       " ['started'],\n",
       " ['postgresql'],\n",
       " ['command'],\n",
       " ['line'],\n",
       " ['...'],\n",
       " ['ibm'],\n",
       " ['blockchain'],\n",
       " ['foundation'],\n",
       " ['developer'],\n",
       " ['dive'],\n",
       " ['deepe'],\n",
       " ['...'],\n",
       " ['keys'],\n",
       " ['constraints'],\n",
       " ['mysql'],\n",
       " ['project'],\n",
       " ['...'],\n",
       " ['exploring'],\n",
       " ['spark'],\n",
       " ['graphx'],\n",
       " ['spark'],\n",
       " ['provides'],\n",
       " ['grap'],\n",
       " ['...'],\n",
       " ['data'],\n",
       " ['science'],\n",
       " ['health'],\n",
       " ['care'],\n",
       " ['basic'],\n",
       " ['statistical'],\n",
       " ['...'],\n",
       " ['create'],\n",
       " ['tables'],\n",
       " ['data'],\n",
       " ['postgresql'],\n",
       " ['usin'],\n",
       " ['...'],\n",
       " ['monitoring'],\n",
       " ['metrics'],\n",
       " ['java'],\n",
       " ['microservices'],\n",
       " ['u'],\n",
       " ['...'],\n",
       " ['configuring'],\n",
       " ['microservices'],\n",
       " ['running'],\n",
       " ['kubernete'],\n",
       " ['...'],\n",
       " ['game'],\n",
       " ['playing'],\n",
       " ['ai'],\n",
       " ['swift'],\n",
       " ['tensorflow'],\n",
       " ['s4t'],\n",
       " ['...'],\n",
       " ['enabling'],\n",
       " ['cross'],\n",
       " ['origin'],\n",
       " ['resource'],\n",
       " ['sharing'],\n",
       " ['cors'],\n",
       " ['...'],\n",
       " ['getting'],\n",
       " ['started'],\n",
       " ['db2'],\n",
       " ['cloud'],\n",
       " ['proj'],\n",
       " ['...'],\n",
       " ['apache'],\n",
       " ['pig'],\n",
       " ['pig'],\n",
       " ['developed'],\n",
       " ['...'],\n",
       " ['developing'],\n",
       " ['distributed'],\n",
       " ['applications'],\n",
       " ['using'],\n",
       " ['zook'],\n",
       " ['...'],\n",
       " ['controlling'],\n",
       " ['hadoop'],\n",
       " ['jobs'],\n",
       " ['using'],\n",
       " ['oozie'],\n",
       " ['apach'],\n",
       " ['...'],\n",
       " ['moving'],\n",
       " ['data'],\n",
       " ['hadoop'],\n",
       " ['course'],\n",
       " ['describes'],\n",
       " ['...'],\n",
       " ['mapreduce'],\n",
       " ['yarn'],\n",
       " ['string'],\n",
       " ['unders'],\n",
       " ['...'],\n",
       " ['deploy'],\n",
       " ['web'],\n",
       " ['server'],\n",
       " ['using'],\n",
       " ['python'],\n",
       " ['ibm'],\n",
       " ['cloud'],\n",
       " ['...'],\n",
       " ['acknowledging'],\n",
       " ['messages'],\n",
       " ['using'],\n",
       " ['microprofile'],\n",
       " ['reac'],\n",
       " ['...'],\n",
       " ['build'],\n",
       " ['personal'],\n",
       " ['movie'],\n",
       " ['recommender'],\n",
       " ['django'],\n",
       " ['...'],\n",
       " ['managing'],\n",
       " ['injecting'],\n",
       " ['dependencies'],\n",
       " ['java'],\n",
       " ['...'],\n",
       " ['spark'],\n",
       " ['fundamentals'],\n",
       " ['ignite'],\n",
       " ['interest'],\n",
       " ['...'],\n",
       " ['deploying'],\n",
       " ['microservices'],\n",
       " ['kubernetes'],\n",
       " ['deploy'],\n",
       " ['...'],\n",
       " ['getting'],\n",
       " ['started'],\n",
       " ['open'],\n",
       " ['liberty'],\n",
       " ['learn'],\n",
       " ['...'],\n",
       " ['consuming'],\n",
       " ['restful'],\n",
       " ['java'],\n",
       " ['microservices'],\n",
       " ['asynchron'],\n",
       " ['...'],\n",
       " ['statistics'],\n",
       " ['take'],\n",
       " ['course'],\n",
       " ['...'],\n",
       " ['analyzing'],\n",
       " ['big'],\n",
       " ['data'],\n",
       " ['spreadsheet'],\n",
       " ['ui'],\n",
       " ['...'],\n",
       " ['openrefine'],\n",
       " ['introduction'],\n",
       " ['course'],\n",
       " ['...'],\n",
       " ['solr'],\n",
       " ['course'],\n",
       " ['introduce'],\n",
       " ['solr'],\n",
       " ['deep'],\n",
       " ['learning'],\n",
       " ['tensorflow'],\n",
       " ['majority'],\n",
       " ['data'],\n",
       " ['...'],\n",
       " ['building'],\n",
       " ['fault'],\n",
       " ['tolerant'],\n",
       " ['microservices'],\n",
       " ['...'],\n",
       " ['consuming'],\n",
       " ['restful'],\n",
       " ['java'],\n",
       " ['microservices'],\n",
       " ['temp'],\n",
       " ['...'],\n",
       " ['build'],\n",
       " ['chatbots'],\n",
       " ['watson'],\n",
       " ['assistant'],\n",
       " ['c'],\n",
       " ['...'],\n",
       " ['red'],\n",
       " ['basics'],\n",
       " ['bots'],\n",
       " ['create'],\n",
       " ['cognitive'],\n",
       " ['web'],\n",
       " ['...'],\n",
       " ['normalization'],\n",
       " ['keys'],\n",
       " ['constraints'],\n",
       " ['relation'],\n",
       " ['...'],\n",
       " ['ibm'],\n",
       " ['cloud'],\n",
       " ['essentials'],\n",
       " ['ibm'],\n",
       " ['cloud'],\n",
       " ['name'],\n",
       " ['...'],\n",
       " ['getting'],\n",
       " ['started'],\n",
       " ['microservices'],\n",
       " ['istio'],\n",
       " ['...'],\n",
       " ['playing'],\n",
       " ['tictactoe'],\n",
       " ['reinforcement'],\n",
       " ['learning'],\n",
       " ['...'],\n",
       " ['deploying'],\n",
       " ['microservice'],\n",
       " ['openshift'],\n",
       " ['using'],\n",
       " ['...'],\n",
       " ['data'],\n",
       " ['analysis'],\n",
       " ['demos'],\n",
       " ['course'],\n",
       " ['provi'],\n",
       " ['...'],\n",
       " ['data'],\n",
       " ['journalism'],\n",
       " ['first'],\n",
       " ['steps'],\n",
       " ['skills'],\n",
       " ['tools'],\n",
       " ['...'],\n",
       " ['mathematical'],\n",
       " ['optimization'],\n",
       " ['business'],\n",
       " ['problem'],\n",
       " ['...'],\n",
       " ['db2'],\n",
       " ['academic'],\n",
       " ['training'],\n",
       " ['db2'],\n",
       " ['academic'],\n",
       " ['training'],\n",
       " ['db2'],\n",
       " ['fundamentals'],\n",
       " ['ii'],\n",
       " ['db2'],\n",
       " ['fundamentals'],\n",
       " ['ii'],\n",
       " ['scalable'],\n",
       " ['web'],\n",
       " ['applications'],\n",
       " ['kubernetes'],\n",
       " ['...'],\n",
       " ['data'],\n",
       " ['analysis'],\n",
       " ['using'],\n",
       " ['r'],\n",
       " ['data'],\n",
       " ['analysis'],\n",
       " ['using'],\n",
       " ['...'],\n",
       " ['nosql'],\n",
       " ['dbaas'],\n",
       " ['nosql'],\n",
       " ['course'],\n",
       " ['w'],\n",
       " ['...'],\n",
       " ['kubernetes'],\n",
       " ['operators'],\n",
       " ['advanced'],\n",
       " ['course'],\n",
       " ['cove'],\n",
       " ['...'],\n",
       " ['project'],\n",
       " ['deploy'],\n",
       " ['serverless'],\n",
       " ['app'],\n",
       " ['image'],\n",
       " ['...'],\n",
       " ['data'],\n",
       " ['science'],\n",
       " ['career'],\n",
       " ['talks'],\n",
       " ['data'],\n",
       " ['science'],\n",
       " ['career'],\n",
       " ['...'],\n",
       " ['data'],\n",
       " ['science'],\n",
       " ['open'],\n",
       " ['data'],\n",
       " ['data'],\n",
       " ['science'],\n",
       " ['...'],\n",
       " ['data'],\n",
       " ['science'],\n",
       " ['bootcamp'],\n",
       " ['python'],\n",
       " ['universi'],\n",
       " ['...'],\n",
       " ['bitcoin'],\n",
       " ['greetings'],\n",
       " ['welcome'],\n",
       " ['intro'],\n",
       " ['...'],\n",
       " ['data'],\n",
       " ['science'],\n",
       " ['hands'],\n",
       " ['open'],\n",
       " ['source'],\n",
       " ['tools'],\n",
       " ['w'],\n",
       " ['...'],\n",
       " ['data'],\n",
       " ['science'],\n",
       " ['methodology'],\n",
       " ['grab'],\n",
       " ['lab'],\n",
       " ['coat'],\n",
       " ['...'],\n",
       " ['creating'],\n",
       " ['asynchronous'],\n",
       " ['java'],\n",
       " ['microservices'],\n",
       " ['using'],\n",
       " ['...'],\n",
       " ['build'],\n",
       " ['smart'],\n",
       " ['search'],\n",
       " ['form'],\n",
       " ['algolia'],\n",
       " ['great'],\n",
       " ['...'],\n",
       " ['documenting'],\n",
       " ['restful'],\n",
       " ['apis'],\n",
       " ['using'],\n",
       " ['microprofile'],\n",
       " ['op'],\n",
       " ['...'],\n",
       " ['reactive'],\n",
       " ['architecture'],\n",
       " ['distributed'],\n",
       " ['messaging'],\n",
       " ['p'],\n",
       " ['...'],\n",
       " ['data'],\n",
       " ['science'],\n",
       " ['agriculture'],\n",
       " ['land'],\n",
       " ['use'],\n",
       " ['classifi'],\n",
       " ['...'],\n",
       " ['reactive'],\n",
       " ['architecture'],\n",
       " ['cqrs'],\n",
       " ['event'],\n",
       " ['sourcing'],\n",
       " ['r'],\n",
       " ['...'],\n",
       " ['deep'],\n",
       " ['learning'],\n",
       " ['deep'],\n",
       " ['learning'],\n",
       " ['reactive'],\n",
       " ['architecture'],\n",
       " ['reactive'],\n",
       " ['microservices'],\n",
       " ['...'],\n",
       " ['reactive'],\n",
       " ['architecture'],\n",
       " ['domain'],\n",
       " ['driven'],\n",
       " ['design'],\n",
       " ['...'],\n",
       " ['building'],\n",
       " ['robots'],\n",
       " ['tjbot'],\n",
       " ['learn'],\n",
       " ['progra'],\n",
       " ['...'],\n",
       " ['building'],\n",
       " ['cloud'],\n",
       " ['native'],\n",
       " ['multicloud'],\n",
       " ['applicati'],\n",
       " ['...'],\n",
       " ['big'],\n",
       " ['data'],\n",
       " ['big'],\n",
       " ['big'],\n",
       " ['big'],\n",
       " ['...'],\n",
       " ['modernizing'],\n",
       " ['java'],\n",
       " ['ee'],\n",
       " ['applications'],\n",
       " ['learn'],\n",
       " ['...'],\n",
       " ['data'],\n",
       " ['science'],\n",
       " ['agriculture'],\n",
       " ['prognostication'],\n",
       " ['u'],\n",
       " ['...'],\n",
       " ['docker'],\n",
       " ['essentials'],\n",
       " ['developer'],\n",
       " ['introduction'],\n",
       " ['le'],\n",
       " ['...'],\n",
       " ['accelerating'],\n",
       " ['deep'],\n",
       " ['learning'],\n",
       " ['gpus'],\n",
       " ['training'],\n",
       " ['...'],\n",
       " ['reactive'],\n",
       " ['architecture'],\n",
       " ['building'],\n",
       " ['scalable'],\n",
       " ['syste'],\n",
       " ['...'],\n",
       " ['build'],\n",
       " ['chatbots'],\n",
       " ['learn'],\n",
       " ['build'],\n",
       " ['cha'],\n",
       " ['...'],\n",
       " ['data'],\n",
       " ['science'],\n",
       " ['agriculture'],\n",
       " ['basic'],\n",
       " ['statistical'],\n",
       " ['...'],\n",
       " ['machine'],\n",
       " ['learning'],\n",
       " ['python'],\n",
       " ['phrases'],\n",
       " ['â€š'],\n",
       " ['...'],\n",
       " ['build'],\n",
       " ['chatbot'],\n",
       " ['build'],\n",
       " ['chat'],\n",
       " ['...'],\n",
       " ['machine'],\n",
       " ['learning'],\n",
       " ['dimensionality'],\n",
       " ['reduction'],\n",
       " ['...'],\n",
       " ['machine'],\n",
       " ['learning'],\n",
       " ['python'],\n",
       " ['machine'],\n",
       " ['learning'],\n",
       " ['...'],\n",
       " ['consuming'],\n",
       " ['restful'],\n",
       " ['java'],\n",
       " ['web'],\n",
       " ['service'],\n",
       " ['using'],\n",
       " ['jso'],\n",
       " ['...'],\n",
       " ['introduction'],\n",
       " ['ibm'],\n",
       " ['cloud'],\n",
       " ['financial'],\n",
       " ['ser'],\n",
       " ['...'],\n",
       " ['data'],\n",
       " ['analysis'],\n",
       " ['python'],\n",
       " ['course'],\n",
       " ['w'],\n",
       " ['...'],\n",
       " ...]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Tokenized_Courses = [word_tokenize(course) for course in tokenized]\n",
    "Tokenized_Courses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "44686fa2-9976-4f78-95ba-5c644f63737a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'robots': 0, 'coming': 1, 'build': 2, 'iot': 3, 'apps': 4, 'watson': 5, '...': 6, 'accelerating': 7, 'deep': 8, 'learning': 9, 'gpu': 10, 'training': 11, 'c': 12, 'consuming': 13, 'restful': 14, 'services': 15, 'using': 16, 'reactive': 17, 'analyzing': 18, 'big': 19, 'data': 20, 'r': 21, 'apache': 22, 'spark': 23, 'apa': 24, 'containerizing': 25, 'packaging': 26, 'running': 27, 'sprin': 28, 'cloud': 29, 'native': 30, 'security': 31, 'conference': 32, 'securi': 33, 'science': 34, 'bootcamp': 35, 'university': 36, 'pr': 37, 'learn': 38, 'use': 39, 'docker': 40, 'containers': 41, 'iterati': 42, 'scorm': 43, 'test': 44, 'scron': 45, 'course': 46, 'create': 47, 'first': 48, 'mongodb': 49, 'database': 50, 'gui': 51, 'testing': 52, 'microservices': 53, 'arquillian': 54, 'mana': 55, 'pak': 56, 'integration': 57, 'essentials': 58, 'analytics': 59, 'social': 60, 'media': 61, 'analy': 62, 'python': 63, 'universi': 64, 'cryptocurrency': 65, 'trading': 66, 'algorithm': 67, 'p': 68, 'ai': 69, 'co': 70, 'securing': 71, 'java': 72, 'eclipse': 73, 'micro': 74, 'enabling': 75, 'distributed': 76, 'tracing': 77, 'sql': 78, 'access': 79, 'hadoop': 80, 'another': 81, 'tool': 82, 'hybrid': 83, 'pipelines': 84, 'lab': 85, 'hybr': 86, 'dataops': 87, 'methodology': 88, 'ops': 89, 'jumpstart': 90, 'journey': 91, 'introduce': 92, 'yo': 93, 'introduction': 94, 'open': 95, 'source': 96, 'introd': 97, 'end': 98, 'cloudpak': 99, 'e': 100, 'everyone': 101, 'master': 102, 'basics': 103, 'serverless': 104, 'computing': 105, 'functions': 106, 'predicting': 107, 'customer': 108, 'satisfaction': 109, 'predict': 110, 'custo': 111, 'validating': 112, 'constraints': 113, 'javabeans': 114, 'getting': 115, 'started': 116, 'financial': 117, 'performance': 118, 'company': 119, 'db2': 120, 'fundamentals': 121, 'clustering': 122, 'methods': 123, 'investment': 124, 'portfo': 125, 'php': 126, 'web': 127, 'application': 128, 'lamp': 129, 'stack': 130, 'tutor': 131, 'javascript': 132, 'rock': 133, 'paper': 134, 'databases': 135, 'intent': 136, 'cour': 137, 'ii': 138, 'building': 139, 'foundat': 140, 'insurance': 141, 'basic': 142, 'statistical': 143, 'applicati': 144, 'overview': 145, 'scala': 146, 'â€š': 147, 'spar': 148, 'microprofile': 149, 'jakarta': 150, 'ee': 151, 'mast': 152, 'text': 153, 'analyze': 154, 'ana': 155, 'swift': 156, 'apis': 157, 'make': 158, 'multi': 159, 'day': 160, 'intensive': 161, 'risk': 162, 'assessment': 163, 'montecarlo': 164, 'meth': 165, 'statistics': 166, 'courses': 167, 'teaching': 168, 'bas': 169, 'adv': 170, 'mobile': 171, 'service': 172, 'analysis': 173, 'various': 174, 'machine': 175, 'sound': 176, 'ge': 177, 'hbase': 178, 'real': 179, 'time': 180, 'welcome': 181, 'analyti': 182, 'business': 183, 'modelling': 184, 'actuari': 185, 'accessing': 186, 'hive': 187, 'dat': 188, 'istio': 189, 'ibm': 190, 'kuberne': 191, 'majority': 192, 'emails': 193, 'blo': 194, 'scale': 195, 'continuation': 196, 'systemml': 197, 'action': 198, 'classification': 199, 'task': 200, 'based': 201, 'internet': 202, 'f': 203, 'container': 204, 'kubernetes': 205, 'clo': 206, 'health': 207, 'care': 208, 'advanced': 209, 'prognost': 210, 'spam': 211, 'clas': 212, 'visual': 213, 'banking': 214, 'maste': 215, 'secure': 216, 'credit': 217, 'card': 218, 'dataset': 219, 'network': 220, 'traffic': 221, 'anomaly': 222, 'detection': 223, 'intrusion': 224, 'node': 225, 'js': 226, 'heard': 227, 'mysql': 228, 'command': 229, 'line': 230, 'thi': 231, 'satellite': 232, 'short': 233, 'quantum': 234, 'launch': 235, 'hotdog': 236, 'detector': 237, 'exploratory': 238, 'eda': 239, 'pandas': 240, 'news': 241, 'tables': 242, 'relational': 243, 'model': 244, 'concepts': 245, 'project': 246, 'predictive': 247, 'modeling': 248, 'blockchain': 249, 'supply': 250, 'understand': 251, 'te': 252, 'postgresql': 253, 'foundation': 254, 'developer': 255, 'dive': 256, 'deepe': 257, 'keys': 258, 'exploring': 259, 'graphx': 260, 'provides': 261, 'grap': 262, 'usin': 263, 'monitoring': 264, 'metrics': 265, 'u': 266, 'configuring': 267, 'kubernete': 268, 'game': 269, 'playing': 270, 'tensorflow': 271, 's4t': 272, 'cross': 273, 'origin': 274, 'resource': 275, 'sharing': 276, 'cors': 277, 'proj': 278, 'pig': 279, 'developed': 280, 'developing': 281, 'applications': 282, 'zook': 283, 'controlling': 284, 'jobs': 285, 'oozie': 286, 'apach': 287, 'moving': 288, 'describes': 289, 'mapreduce': 290, 'yarn': 291, 'string': 292, 'unders': 293, 'deploy': 294, 'server': 295, 'acknowledging': 296, 'messages': 297, 'reac': 298, 'personal': 299, 'movie': 300, 'recommender': 301, 'django': 302, 'managing': 303, 'injecting': 304, 'dependencies': 305, 'ignite': 306, 'interest': 307, 'deploying': 308, 'liberty': 309, 'asynchron': 310, 'take': 311, 'spreadsheet': 312, 'ui': 313, 'openrefine': 314, 'solr': 315, 'fault': 316, 'tolerant': 317, 'temp': 318, 'chatbots': 319, 'assistant': 320, 'red': 321, 'bots': 322, 'cognitive': 323, 'normalization': 324, 'relation': 325, 'name': 326, 'tictactoe': 327, 'reinforcement': 328, 'microservice': 329, 'openshift': 330, 'demos': 331, 'provi': 332, 'journalism': 333, 'steps': 334, 'skills': 335, 'tools': 336, 'mathematical': 337, 'optimization': 338, 'problem': 339, 'academic': 340, 'scalable': 341, 'nosql': 342, 'dbaas': 343, 'w': 344, 'operators': 345, 'cove': 346, 'app': 347, 'image': 348, 'career': 349, 'talks': 350, 'bitcoin': 351, 'greetings': 352, 'intro': 353, 'hands': 354, 'grab': 355, 'coat': 356, 'creating': 357, 'asynchronous': 358, 'smart': 359, 'search': 360, 'form': 361, 'algolia': 362, 'great': 363, 'documenting': 364, 'op': 365, 'architecture': 366, 'messaging': 367, 'agriculture': 368, 'land': 369, 'classifi': 370, 'cqrs': 371, 'event': 372, 'sourcing': 373, 'domain': 374, 'driven': 375, 'design': 376, 'tjbot': 377, 'progra': 378, 'multicloud': 379, 'modernizing': 380, 'prognostication': 381, 'le': 382, 'gpus': 383, 'syste': 384, 'cha': 385, 'phrases': 386, 'chatbot': 387, 'chat': 388, 'dimensionality': 389, 'reduction': 390, 'jso': 391, 'ser': 392, 'micr': 393, 'checking': 394, 'typesafe': 395, 'scie': 396, 'train': 397, 'recognition': 398, 'py': 399, 'externalizing': 400, 'configuration': 401, 'microserv': 402, 'integrating': 403, 'cql': 404, 'shell': 405, 'execute': 406, 'keyspace': 407, 'operat': 408, 'performing': 409, 'table': 410, 'crud': 411, 'operations': 412, 'cass': 413, 'publish': 414, 'guided': 415, 'projects': 416, 'simple': 417, 'backend': 418, 'transform': 419, 'photos': 420, 'sketches': 421, 'paintings': 422, 'wit': 423, 'quick': 424, 'b': 425, 'experimental': 426, 'art': 427, 'uncove': 428, 'reactiv': 429, 'privacy': 430, 'fundame': 431, 'angu': 432, 'implement': 433, 'consumer': 434, 'contract': 435, 'free': 436, 'intr': 437, 'introduces': 438, 'yelp': 439, 'reviews': 440, 'sentiment': 441, 'mllib': 442, 'v3': 443, 'introduc': 444, 'cloudant': 445, 'working': 446, 'beginner': 447, 'simplifying': 448, 'kafka': 449, 'ready': 450, 'medical': 451, 'appointment': 452, 'note': 453, 'visualization': 454, 'implementing': 455, 'graphql': 456, 'wi': 457, 'hypermedia': 458, 'visualizat': 459, 'digital': 460, 'regression': 461, 'head': 462, 'persisting': 463, 'prognostica': 464, 'powered': 465, 'discord': 466, 'bot': 467, 'voice': 468, 'views': 469, 'streaming': 470, 'updates': 471, 'fast': 472, 'systems': 473, 'spe': 474, 'intelligence': 475, 'warehousing': 476, 'wel': 477, 'specialization': 478, '\\\\nsql': 479, 'collection': 480, '\\\\ndistributed': 481, 'capstone': 482, 'management': 483, 'manage': 484, 'process': 485, 'dirty': 486, 'clean': 487, 'cours': 488, 'nature': 489, 'des': 490, 'scripting': 491, 'files': 492, 'inheritance': 493, 'support': 494, 'warehouse': 495, 'structured': 496, 'query': 497, 'language': 498, 'crash': 499, 'designed': 500, 'programming': 501, 'everybody': 502, 'concise': 503, 'applied': 504, 'provide': 505, 'foundations': 506, 'fir': 507, 'part': 508, 'computi': 509, 'sy': 510, 'pre': 511, 'excel': 512, 'dataframes': 513, 'virtualization': 514, 'welc': 515, 'alibaba': 516, 'solutions': 517, 'scien': 518, 'oft': 519, 'ml': 520, 'production': 521, 'lifecycle': 522, 'models': 523, 'productio': 524, 'computer': 525, 'vision': 526, 'th': 527, 'video': 528, 'proces': 529, 'artificial': 530, 'inte': 531, 'convolutional': 532, 'neural': 533, 'networks': 534, 'life': 535, 'analysts': 536, 'executive': 537, 'interested': 538, 'increas': 539, 'want': 540, 'se': 541, 'cap': 542, 'hdfs': 543, 'fundamenta': 544, 'platform': 545, 'framework': 546, 'proje': 547, 'natural': 548, 'processing': 549, 'attention': 550, 'mod': 551, 'sequence': 552, 'mode': 553, 'probabilistic': 554, 'ho': 555, 'probability': 556, 'ggplot2': 557, 'visu': 558, 'environment': 559, 'prov': 560, 'html': 561, 'css': 562, 'developers': 563, 'jquery': 564, 'json': 565, 'front': 566, 'development': 567, 'react': 568, 'interactivity': 569}\n"
     ]
    }
   ],
   "source": [
    "# WRITE YOUR CODE HERE\n",
    "TOKENS_DICT = gensim.corpora.Dictionary(Tokenized_Courses)\n",
    "print(TOKENS_DICT.token2id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e844282-a017-4bfd-8578-eaf6148b855d",
   "metadata": {},
   "source": [
    "Then we can use `doc2bow()` method to generate BoW features for each tokenized course.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ec8d96b-78ad-4e1e-bce4-395555ae840b",
   "metadata": {},
   "source": [
    "_TODO: Use tokens_dict.doc2bow() to generate BoW features for each tokenized course._\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "acedc4df-22f4-4f7f-a19f-36ab346d707c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [(12, 1)],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [(3, 1)],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [(3, 1)],\n",
       " [],\n",
       " [],\n",
       " [(3, 1)],\n",
       " [(7, 1)],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [(2, 1)],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [(3, 1)],\n",
       " [(7, 1)],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [(3, 1)],\n",
       " [],\n",
       " [],\n",
       " [(3, 1)],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [(3, 1)],\n",
       " [],\n",
       " [(2, 1)],\n",
       " [(3, 1)],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [(5, 1)],\n",
       " [],\n",
       " [],\n",
       " [(2, 1)],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [(3, 1)],\n",
       " [(7, 1)],\n",
       " [],\n",
       " [(3, 1)],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [(3, 1)],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [(2, 1)],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [(3, 1)],\n",
       " [(7, 1)],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [(5, 1)],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [(3, 1)],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [(3, 1)],\n",
       " [(7, 1)],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [(3, 1)],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [(5, 1)],\n",
       " [(13, 1)],\n",
       " [(12, 1)],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [(3, 1)],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [(12, 1)],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [(3, 1)],\n",
       " [(7, 1)],\n",
       " [],\n",
       " [],\n",
       " [(3, 1)],\n",
       " [(7, 1)],\n",
       " [],\n",
       " [(13, 1)],\n",
       " [(12, 1)],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [(3, 1)],\n",
       " [(7, 1)],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [(13, 1)],\n",
       " [],\n",
       " [(12, 1)],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [(3, 1)],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [(3, 1)],\n",
       " [(7, 1)],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [(13, 1)],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [(5, 1)],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [(5, 1)],\n",
       " [],\n",
       " [],\n",
       " [(2, 1)],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [(3, 1)],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [(3, 1)],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [(3, 1)],\n",
       " [(7, 1)],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [(3, 1)],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [(3, 1)],\n",
       " [],\n",
       " [(2, 1)],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [(2, 1)],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [(3, 1)],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [(5, 1)],\n",
       " [(2, 1)],\n",
       " [],\n",
       " [],\n",
       " [(2, 1)],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [(12, 1)],\n",
       " [],\n",
       " [],\n",
       " [(3, 1)],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [(12, 1)],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [(3, 1)],\n",
       " [],\n",
       " [],\n",
       " [(2, 1)],\n",
       " [],\n",
       " [],\n",
       " [(3, 1)],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [(3, 1)],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [(3, 1)],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [(2, 1)],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [(2, 1)],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [(3, 1)],\n",
       " [(7, 1)],\n",
       " [],\n",
       " [],\n",
       " [(3, 1)],\n",
       " [(7, 1)],\n",
       " [],\n",
       " [],\n",
       " [(3, 1)],\n",
       " [(7, 1)],\n",
       " [],\n",
       " [(3, 1)],\n",
       " [(3, 1)],\n",
       " [(7, 1)],\n",
       " [],\n",
       " [(3, 1)],\n",
       " [(7, 1)],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [(3, 1)],\n",
       " [(7, 1)],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [(3, 1)],\n",
       " [(7, 1)],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [(3, 1)],\n",
       " [(7, 1)],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [(12, 1)],\n",
       " [],\n",
       " [(12, 1)],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [(3, 1)],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [(3, 1)],\n",
       " [(7, 1)],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [(5, 1)],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [(12, 1)],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [(3, 1)],\n",
       " [(7, 1)],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [(13, 1)],\n",
       " [(12, 1)],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [(13, 1)],\n",
       " [(12, 1)],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [(13, 1)],\n",
       " [(12, 1)],\n",
       " [],\n",
       " [(13, 1)],\n",
       " [(12, 1)],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [(5, 1)],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [(3, 1)],\n",
       " [],\n",
       " [],\n",
       " [(2, 1)],\n",
       " [],\n",
       " [],\n",
       " ...]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# WRITE YOUR CODE HERE\n",
    "Courses_Bow = [tokens_dict.doc2bow(course) for course in Tokenized_Courses]\n",
    "Courses_Bow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9b01648-53d7-492d-b5e0-eba8f5a50d6c",
   "metadata": {},
   "source": [
    "<details>\n",
    "    <summary>Click here for Hints</summary>\n",
    "    \n",
    "You can use `tokens_dict.doc2bow(course)` command  for each course in `tokenized_courses`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f30dd84-f6e3-4afc-bf65-63e374dd7e90",
   "metadata": {},
   "source": [
    "Lastly, you need to append the BoW features for each course into a new BoW dataframe. The new dataframe needs to include the following columns (you may include other relevant columns as well):\n",
    "- 'doc_index': the course index starting from 0\n",
    "- 'doc_id': the actual course id such as `ML0201EN`\n",
    "- 'token': the tokens for each course\n",
    "- 'bow': the bow value for each token\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2dc4446-cd42-4e12-b812-604229bc4544",
   "metadata": {},
   "source": [
    "_TODO: Create a new course_bow dataframe based on the extracted BoW features._\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "debee268-c31d-441c-9a21-c9025fc8e288",
   "metadata": {},
   "outputs": [],
   "source": [
    "# WRITE YOUR CODE HERE\n",
    "\n",
    "#  ...\n",
    "#  bow_dicts = {\"doc_index\": doc_indices,\n",
    "#            \"doc_id\": doc_ids,\n",
    "#            \"token\": tokens,\n",
    "#            \"bow\": bow_values}\n",
    "#  pd.DataFrame(bow_dicts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf5f4ce8-a48a-495e-825d-98857576749a",
   "metadata": {},
   "source": [
    "<details>\n",
    "    <summary>Click here for Hints</summary>\n",
    "    \n",
    "You can use 2 for-loops to create your data frame: first one will be `for doc_index, doc_bow in enumerate(bow_docs):` where bow_docs is the list of BoW features for each tokenized course and within this for-loop you will have another loop `for token_index, token_bow in doc_bow:`. Then you can get each \"token\" by applying the `token_index` to your `token_dict`,  `token_bow` will give you \"bow\" values, `doc_indices` will give you values for  \"doc_index\" and you can get \"doc_id\" by using `courses_df['COURSE_ID']` list and `doc_index` as indexes.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62552290-8e53-4e5d-9fb1-6abdec875b08",
   "metadata": {},
   "source": [
    "Your course BoW dataframe may look like the following:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14cc5ac0-bd11-485a-9ef1-fb40101676d8",
   "metadata": {},
   "source": [
    "![](https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBM-ML321EN-SkillsNetwork/labs/module_2/images/bow_dataset.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7eee1a8d-fb3e-48bb-b1a6-65fe7745b321",
   "metadata": {},
   "source": [
    "You may refer to previous code examples in this lab if you need help with creating the BoW dataframe.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f99350d3-7109-4831-bb88-e2dfe42584c6",
   "metadata": {},
   "source": [
    "### Other popular textual features\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48281f28-44b4-476d-8fb0-55ec2707d6ce",
   "metadata": {},
   "source": [
    "In addition to the basic token BoW feature, there are two other types of widely used textual features. If you are interested, you may explore them yourself to learn how to extract them from the course textual content: \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f03d93c-acde-4228-aa3f-706d399a4664",
   "metadata": {},
   "source": [
    "- **tf-idf**: tf-idf refers to Term Frequencyâ€“Inverse Document Frequency. Similar to BoW, the tf-idf also counts the word frequencies in each document. Furthermore, tf-idf will  offset the number of documents in the corpus that contain the word in order to adjust for the fact that some words appear more frequently in general. The higher the tf-idf normally means the greater the importance the word/token is.\n",
    "- **Text embedding vector**. Embedding means projecting an object into a latent feature space. We normally employ neural networks or deep neural networks to learn the latent features of a textual object such as a word, a sentence, or the entire document. The learned latent feature vectors will be used to represent the original textual entities. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42e466ac-0b96-438e-82be-058905cc2511",
   "metadata": {},
   "source": [
    "### Summary\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ad3b588-9f7d-470a-8152-27a9a167b669",
   "metadata": {},
   "source": [
    "Congratulations, you have completed the BoW feature extraction lab. In this lab, you have learned and practiced extracting BoW features from course titles and descriptions. Once the feature vectors on the courses has been built, we can then apply machine learning algorithms such as similarity measurements, clustering, or classification on the courses in later labs.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efed39d8-a3e7-4054-8314-3b8418b70fd4",
   "metadata": {},
   "source": [
    "## Authors\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93e21023-2ed9-4f3c-af4b-1db5323f0458",
   "metadata": {},
   "source": [
    "[Yan Luo](https://www.linkedin.com/in/yan-luo-96288783/?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMML321ENSkillsNetwork817-2022-01-01)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acf7a0a7-3c03-4854-a12e-01a673576160",
   "metadata": {},
   "source": [
    "### Other Contributors\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03a13659-178e-415d-9b44-990390536d0f",
   "metadata": {},
   "source": [
    "## Change Log\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "203ccbc6-9624-4ba3-96cc-e1adf00c63b4",
   "metadata": {},
   "source": [
    "|Date (YYYY-MM-DD)|Version|Changed By|Change Description|\n",
    "|-|-|-|-|\n",
    "|2021-10-25|1.0|Yan|Created the initial version|\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "870b57df-0496-494a-8c29-c79c16093c15",
   "metadata": {},
   "source": [
    "Copyright Â© 2021 IBM Corporation. All rights reserved.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
